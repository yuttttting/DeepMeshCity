{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c64ea29",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f941ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Dataset_TPE(Dataset):\n",
    "    def __init__(self, configs, mode):\n",
    "        self.datapath = configs.datapath\n",
    "        self.data_type = configs.data_type\n",
    "        self.max_value = configs.max_value\n",
    "        if mode == 'train':\n",
    "            self.name = 'train_' + self.data_type + '.npz'\n",
    "        elif mode == 'valid':\n",
    "            self.name = 'valid_' + self.data_type + '.npz'\n",
    "        else:\n",
    "            self.name = 'test_' + self.data_type + '.npz'\n",
    "        path = os.path.join(self.datapath, self.name)\n",
    "        self.data = self.load_data(path)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        all_data = np.load(path)\n",
    "        self.XC = all_data['xc']\n",
    "        self.XP = all_data['xp']\n",
    "        self.XT = all_data['xt']\n",
    "        self.YS = all_data['ys']\n",
    "        self.YD = all_data['yd']\n",
    "\n",
    "        print(self.XC.shape, self.XP.shape, self.XT.shape, self.YS.shape, self.YD.shape, self.max_value)\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['xc'].shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        XC = torch.FloatTensor(self.XC[item])\n",
    "        XP = torch.FloatTensor(self.XP[item])\n",
    "        XT = torch.FloatTensor(self.XT[item])\n",
    "        YS = torch.FloatTensor(self.YS[item])\n",
    "        YD = torch.FloatTensor(self.YD[item])\n",
    "\n",
    "        return XC, XP, XT, YS, YD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee745b86",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d0e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CGL(nn.Module):\n",
    "    def __init__(self, in_channel, num_hidden, filter_size, stride, is_norm):\n",
    "        super(CGL, self).__init__()\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "        self.padding = filter_size // 2\n",
    "        self.forget_bias = -1.0\n",
    "\n",
    "        if is_norm:\n",
    "            self.conv_x = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, num_hidden * 4, kernel_size=filter_size, stride=stride, padding=self.padding),\n",
    "                nn.GroupNorm(num_hidden // 2, 4 * num_hidden)\n",
    "            )\n",
    "            self.conv_c = nn.Sequential(\n",
    "                nn.Conv2d(num_hidden, num_hidden * 2, kernel_size=filter_size, stride=stride, padding=self.padding),\n",
    "                nn.GroupNorm(num_hidden // 2, num_hidden * 2)\n",
    "            )\n",
    "\n",
    "            self.conv_m = nn.Sequential(\n",
    "                nn.Conv2d(num_hidden, num_hidden * 2, kernel_size=filter_size, stride=stride, padding=self.padding),\n",
    "                nn.GroupNorm(num_hidden // 2, num_hidden * 2)\n",
    "            )\n",
    "            self.conv_o_c = nn.Sequential(\n",
    "                nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=1, stride=1, padding=0),\n",
    "                nn.GroupNorm(num_hidden // 2, num_hidden)\n",
    "            )\n",
    "        else:\n",
    "            self.conv_x = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, num_hidden * 4, kernel_size=filter_size, stride=stride, padding=self.padding),\n",
    "            )\n",
    "            self.conv_c = nn.Sequential(\n",
    "                nn.Conv2d(num_hidden, num_hidden * 2, kernel_size=filter_size, stride=stride, padding=self.padding),\n",
    "            )\n",
    "            self.conv_m = nn.Sequential(\n",
    "                nn.Conv2d(num_hidden, num_hidden * 2, kernel_size=filter_size, stride=stride, padding=self.padding),\n",
    "            )\n",
    "            self.conv_o_c = nn.Sequential(\n",
    "                nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding),\n",
    "            )\n",
    "\n",
    "        self.conv_last = nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x_t, c_t, m_t):\n",
    "        x_concat = self.conv_x(x_t)\n",
    "        m_concat = self.conv_m(m_t)\n",
    "        c_concat = self.conv_c(c_t)\n",
    "\n",
    "        x_p, f_x, x_m_p, f_m_x = torch.split(x_concat, self.num_hidden, dim=1)\n",
    "        f_m, g_m = torch.split(m_concat, self.num_hidden, dim=1)\n",
    "        f_c, g_c = torch.split(c_concat, self.num_hidden, dim=1)\n",
    "\n",
    "        f_t = torch.sigmoid(f_x + f_c + self.forget_bias)\n",
    "        x_p_t = torch.tanh(x_p + g_c)\n",
    "\n",
    "        c = f_t * c_t + (1 - f_t) * x_p_t\n",
    "\n",
    "        f_t_prime = torch.sigmoid(f_m_x + f_m + self.forget_bias)\n",
    "        g_t_prime = torch.tanh(x_m_p + g_m)\n",
    "\n",
    "        m = f_t_prime * m_t + (1 - f_t_prime) * g_t_prime\n",
    "\n",
    "        mem = torch.cat((c, m), 1)\n",
    "\n",
    "        o_t = torch.sigmoid(self.conv_o_c(mem))\n",
    "        h_new = o_t * torch.tanh(self.conv_last(mem))\n",
    "\n",
    "        return h_new, c, m\n",
    "\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_szie=1):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.layer_q = nn.Conv2d(input_dim, hidden_dim, kernel_szie)\n",
    "        self.layer_k = nn.Conv2d(input_dim, hidden_dim, kernel_szie)\n",
    "        self.layer_v = nn.Conv2d(input_dim, hidden_dim, kernel_szie)\n",
    "        self.layer_f = nn.Conv2d(hidden_dim, input_dim, kernel_szie)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, q, k, v, extra=None):\n",
    "        batch_size, channel, H, W = q.shape\n",
    "        h = q  # short connection\n",
    "\n",
    "        q = self.layer_q(q)\n",
    "        k = self.layer_k(k)\n",
    "        v = self.layer_v(v)\n",
    "\n",
    "        if extra is not None:\n",
    "            # 外部特徵 encode 成 attention bias\n",
    "            # extra shape: (B, M, H, W)\n",
    "            extra_flat = extra.view(batch_size, -1, H * W)\n",
    "            q = q + extra_flat.mean(dim=1, keepdim=True).unsqueeze(-1)  # broadcast to q shape\n",
    "            k = k + extra_flat.mean(dim=1, keepdim=True).unsqueeze(-1)\n",
    "\n",
    "        q = q.view(batch_size, self.hidden_dim, H * W)\n",
    "        k = k.view(batch_size, self.hidden_dim, H * W)\n",
    "        v = v.view(batch_size, self.hidden_dim, H * W)\n",
    "\n",
    "        attn = torch.matmul(q.transpose(1, 2), k)\n",
    "        # print('e shape is', e.shape)\n",
    "        attention = torch.softmax(attn, dim=-1)  # attention\n",
    "        z = torch.matmul(attention, v.permute(0, 2, 1))\n",
    "        z = z.view(batch_size, self.hidden_dim, H, W)\n",
    "        out = self.layer_f(z) + h\n",
    "\n",
    "        return out, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d10e6c",
   "metadata": {},
   "source": [
    "# DeepMeshCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afaf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class DeepMeshCity(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(DeepMeshCity, self).__init__()\n",
    "        # hyperparrams\n",
    "        self.meta_in = configs.meta_dim\n",
    "        self.map_width = configs.map_width\n",
    "        self.patch_size = configs.patch_size\n",
    "        self.resize_ratio = self.map_width // self.patch_size\n",
    "        self.frame_channel = configs.map_channel\n",
    "        self.clossness_len = configs.clossness_len\n",
    "        self.periodic_len = configs.periodic_len\n",
    "        self.trend_len = configs.trend_len\n",
    "        self.device = configs.device\n",
    "\n",
    "        # Parameter for External Meta data\n",
    "        self.meta_out = self.map_width * self.map_width * self.frame_channel\n",
    "        self.is_metadate = configs.is_metadate\n",
    "        self.Meta_frame_channel = self.frame_channel * 2 if self.is_metadate else self.frame_channel\n",
    "        self.is_extra = configs.is_extra \n",
    "\n",
    "        # Parameter for Model\n",
    "        self.num_layers = configs.num_layers\n",
    "        self.num_hidden = configs.num_hidden\n",
    "        self.filter_size = configs.filter_size\n",
    "        self.stride = configs.stride\n",
    "        self.layer_norm = configs.layer_norm\n",
    "\n",
    "        # Early Fusion\n",
    "        if self.is_metadate:\n",
    "            self.meta_learn = nn.Sequential(nn.Linear(self.meta_in, 10),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(10, self.meta_out),\n",
    "                                            nn.ReLU())\n",
    "        \n",
    "        # Stacked SA-CGL Blocks\n",
    "        SACGL_blocks = []\n",
    "        for i in range(self.num_layers):\n",
    "            in_channel = self.Meta_frame_channel if i == 0 else self.num_hidden[i - 1]\n",
    "            attn_channel = in_channel * self.resize_ratio * self.resize_ratio\n",
    "            SACGL_blocks.append(nn.ModuleList([\n",
    "                self_attention(attn_channel, self.num_hidden[i]),\n",
    "                CGL(in_channel, self.num_hidden[i], self.filter_size, self.stride, self.layer_norm)\n",
    "            ]))\n",
    "\n",
    "        self.SACGL_blocks = nn.ModuleList(SACGL_blocks)\n",
    "\n",
    "        # Output Module\n",
    "        self.Output_module = nn.Sequential(\n",
    "            nn.Conv2d(self.num_hidden[self.num_layers - 1], self.num_hidden[-1],\n",
    "                      kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.num_hidden[-1], self.frame_channel,\n",
    "                      kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        )\n",
    "    def Attention_module(self, net, i, extra=None):\n",
    "        net = rearrange(net, 'b c (p1 h) (p2 w) -> b (h w c) p1 p2', p1=self.patch_size, p2=self.patch_size)\n",
    "        if extra is not None:\n",
    "            extra = rearrange(extra, 'b c (p1 h) (p2 w) -> b (h w c) p1 p2', p1=self.patch_size, p2=self.patch_size)\n",
    "        \n",
    "        net, _ = self.SACGL_blocks[i][0](net, net, net, extra)\n",
    "        net = rearrange(net, 'b (h w c) p1 p2 -> b c (p1 h) (p2 w)', h=self.resize_ratio, w=self.resize_ratio)\n",
    "\n",
    "        return net\n",
    "\n",
    "    def forward(self, xc, xp, xt, yd):\n",
    "        B, _, C, H, W = xc.shape\n",
    "\n",
    "        if self.is_metadate:\n",
    "            if yd.dim() > 2:\n",
    "                yd = yd.view(yd.shape[0], -1)\n",
    "            \n",
    "            yd = rearrange(self.meta_learn(yd), 'b (f c h w) -> b f c h w', c=self.frame_channel,\n",
    "                           h=self.map_width, w=self.map_width)\n",
    "\n",
    "            xcd = yd.repeat(1, self.clossness_len, 1, 1, 1) if self.clossness_len > 1 else yd\n",
    "            xpd = yd.repeat(1, self.periodic_len, 1, 1, 1) if self.periodic_len > 1 else yd\n",
    "            xtd = yd.repeat(1, self.trend_len, 1, 1, 1) if self.trend_len > 1 else yd\n",
    "\n",
    "            xp = torch.cat((xp, xpd), dim=2)\n",
    "            xt = torch.cat((xt, xtd), dim=2)\n",
    "            xc = torch.cat((xc, xcd), dim=2)\n",
    "\n",
    "        frames = torch.cat((xt, xp, xc), dim=1)\n",
    "\n",
    "        batch = frames.shape[0]\n",
    "        height = frames.shape[-2]\n",
    "        width = frames.shape[-1]\n",
    "\n",
    "        h_t = []\n",
    "        c_t = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            zeros = torch.zeros([batch, self.num_hidden[i], height, width]).to(self.device)\n",
    "            h_t.append(zeros)\n",
    "            c_t.append(zeros)\n",
    "\n",
    "        memory = torch.zeros([batch, self.num_hidden[0], height, width]).to(self.device)\n",
    "\n",
    "        total_length = self.clossness_len + self.periodic_len + self.trend_len\n",
    "        for t in range(total_length):\n",
    "            net = frames[:, t]\n",
    "\n",
    "            net = self.Attention_module(net, 0, extra=self.is_extra)\n",
    "            \n",
    "            h_t[0], c_t[0], memory = self.SACGL_blocks[0][1](net, c_t[0], memory)\n",
    "\n",
    "            for i in range(1, self.num_layers):\n",
    "                h_t[i - 1] = self.Attention_module(h_t[i - 1], i, extra=self.is_extra)\n",
    "                h_t[i], c_t[i], memory = self.SACGL_blocks[i][1](h_t[i - 1], c_t[i], memory)\n",
    "\n",
    "        x_gen = self.Output_module(h_t[self.num_layers - 1])\n",
    "        return x_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b631bd",
   "metadata": {},
   "source": [
    "# model_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6382fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(configs):\n",
    "    networks_map = {\n",
    "        'DeepMeshCity_TPE': DeepMeshCity,\n",
    "    }\n",
    "    model_type = configs.model_type\n",
    "\n",
    "    if model_type in networks_map:\n",
    "        Network = networks_map[model_type]\n",
    "        network = Network(configs).to(configs.device)\n",
    "        return network\n",
    "    else:\n",
    "        raise ValueError('Name of network unknown %s' % model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8c7c8",
   "metadata": {},
   "source": [
    "# grid in csv to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edd98f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grid_271', 'grid_300', 'grid_301', 'grid_188', 'grid_302', 'grid_189', 'grid_303', 'grid_299', 'grid_216', 'grid_217', 'grid_331', 'grid_218', 'grid_215', 'grid_244', 'grid_245', 'grid_246', 'grid_272', 'grid_247', 'grid_273', 'grid_274', 'grid_275', 'grid_243', 'grid_420', 'grid_416', 'grid_417', 'grid_418', 'grid_419', 'grid_447', 'grid_308', 'grid_304', 'grid_305', 'grid_306', 'grid_307', 'grid_392', 'grid_388', 'grid_389', 'grid_390', 'grid_391', 'grid_386', 'grid_387', 'grid_414', 'grid_415', 'grid_276', 'grid_277', 'grid_278', 'grid_279', 'grid_364', 'grid_360', 'grid_361', 'grid_362', 'grid_363', 'grid_358', 'grid_359', 'grid_248', 'grid_249', 'grid_250', 'grid_332', 'grid_333', 'grid_334', 'grid_335', 'grid_330', 'grid_448', 'grid_220', 'grid_475', 'grid_221', 'grid_336', 'grid_219', 'grid_238', 'grid_239', 'grid_266', 'grid_267', 'grid_296', 'grid_184', 'grid_185', 'grid_186', 'grid_187', 'grid_294', 'grid_295', 'grid_183', 'grid_324', 'grid_212', 'grid_213', 'grid_214', 'grid_323', 'grid_210', 'grid_211', 'grid_240', 'grid_241', 'grid_242', 'grid_268', 'grid_128', 'grid_129', 'grid_130', 'grid_156', 'grid_127', 'grid_157', 'grid_158', 'grid_155', 'grid_160', 'grid_161', 'grid_159', 'grid_190', 'grid_191', 'grid_132', 'grid_133', 'grid_298', 'grid_328', 'grid_329', 'grid_326', 'grid_327', 'grid_356', 'grid_357', 'grid_354', 'grid_355', 'grid_384', 'grid_385', 'grid_383', 'grid_341', 'grid_342', 'grid_343', 'grid_338', 'grid_339', 'grid_40', 'grid_41', 'grid_292', 'grid_293', 'grid_39', 'grid_288', 'grid_289', 'grid_290', 'grid_291', 'grid_316', 'grid_317', 'grid_318', 'grid_319', 'grid_312', 'grid_313', 'grid_314', 'grid_315', 'grid_310', 'grid_311', 'grid_12', 'grid_13', 'grid_264', 'grid_265', 'grid_11', 'grid_260', 'grid_261', 'grid_262', 'grid_263', 'grid_258', 'grid_259', 'grid_284', 'grid_285', 'grid_286', 'grid_287', 'grid_236', 'grid_237', 'grid_232', 'grid_233', 'grid_234', 'grid_235', 'grid_204', 'grid_205', 'grid_206', 'grid_207', 'grid_208', 'grid_209', 'grid_429', 'grid_430', 'grid_180', 'grid_181', 'grid_182', 'grid_178', 'grid_179', 'grid_153', 'grid_154', 'grid_400', 'grid_401', 'grid_402', 'grid_403', 'grid_96', 'grid_97', 'grid_98', 'grid_124', 'grid_125', 'grid_126', 'grid_376', 'grid_372', 'grid_373', 'grid_374', 'grid_375', 'grid_370', 'grid_371', 'grid_68', 'grid_69', 'grid_320', 'grid_321', 'grid_67', 'grid_348', 'grid_95', 'grid_344', 'grid_345', 'grid_346', 'grid_347', 'grid_340', 'grid_297', 'grid_325', 'grid_269', 'grid_270', 'grid_440', 'grid_441', 'grid_442', 'grid_437', 'grid_580', 'grid_438', 'grid_581', 'grid_439', 'grid_496', 'grid_497', 'grid_498', 'grid_412', 'grid_413', 'grid_552', 'grid_409', 'grid_553', 'grid_410', 'grid_411', 'grid_468', 'grid_469', 'grid_470', 'grid_465', 'grid_608', 'grid_609', 'grid_466', 'grid_467', 'grid_381', 'grid_524', 'grid_525', 'grid_382', 'grid_162', 'grid_76', 'grid_77', 'grid_75', 'grid_104', 'grid_105', 'grid_106', 'grid_107', 'grid_103', 'grid_134', 'grid_135', 'grid_131', 'grid_36', 'grid_37', 'grid_38', 'grid_63', 'grid_9', 'grid_10', 'grid_256', 'grid_257', 'grid_281', 'grid_282', 'grid_283', 'grid_228', 'grid_229', 'grid_230', 'grid_231', 'grid_226', 'grid_227', 'grid_253', 'grid_254', 'grid_255', 'grid_200', 'grid_201', 'grid_202', 'grid_203', 'grid_199', 'grid_172', 'grid_173', 'grid_174', 'grid_175', 'grid_176', 'grid_177', 'grid_152', 'grid_148', 'grid_149', 'grid_150', 'grid_151', 'grid_145', 'grid_146', 'grid_147', 'grid_120', 'grid_121', 'grid_122', 'grid_123', 'grid_117', 'grid_118', 'grid_119', 'grid_64', 'grid_65', 'grid_66', 'grid_92', 'grid_93', 'grid_94', 'grid_90', 'grid_91', 'grid_436', 'grid_432', 'grid_433', 'grid_434', 'grid_435', 'grid_408', 'grid_404', 'grid_405', 'grid_406', 'grid_407', 'grid_489', 'grid_490', 'grid_491', 'grid_352', 'grid_353', 'grid_380', 'grid_377', 'grid_378', 'grid_379', 'grid_460', 'grid_461', 'grid_462', 'grid_463', 'grid_322', 'grid_349', 'grid_350', 'grid_351', 'grid_464']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假設你的檔案叫 tpe_grids.csv\n",
    "df = pd.read_csv(\"dataset_4yr/TPE_113_grid.csv\")\n",
    "\n",
    "# 取出所有 grid 欄位 (去掉 'index')\n",
    "grid_columns = [col for col in df.columns if col.startswith(\"grid_\")]\n",
    "print(grid_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb35109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_id_to_index(grid_id, ncols=28):\n",
    "    # 假設 grid_id 是 0 開始編號的 28*28 flatten\n",
    "    row = grid_id // ncols\n",
    "    col = grid_id % ncols\n",
    "    return row, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4613788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 建立一個 mask (28x28)\n",
    "mask = np.zeros((28, 28), dtype=bool)\n",
    "count = 0\n",
    "for col in grid_columns:\n",
    "    grid_id = int(col.replace(\"grid_\", \"\"))\n",
    "    grid_id = grid_id - 1  # 假設 grid_id 從 1 開始，轉換為 0 開始\n",
    "    row, col = grid_id_to_index(grid_id)\n",
    "    mask[col, row] = True\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77c1a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21488 (\\N{CJK UNIFIED IDEOGRAPH-53F0}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21271 (\\N{CJK UNIFIED IDEOGRAPH-5317}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24066 (\\N{CJK UNIFIED IDEOGRAPH-5E02}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26377 (\\N{CJK UNIFIED IDEOGRAPH-6709}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 25928 (\\N{CJK UNIFIED IDEOGRAPH-6548}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 32178 (\\N{CJK UNIFIED IDEOGRAPH-7DB2}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26684 (\\N{CJK UNIFIED IDEOGRAPH-683C}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApMAAAKqCAYAAABmY9WYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAf4VJREFUeJzt3Q1clfX9//E3oBxvSiWw1OU9I1AE736uMHMzp9VmGQ4327Ici5rWZlN/jMU0au1Ua7P1a6abYqZOp3mbudRKyy22vAualuZwYbMUCSENweD8H9+rv8RJuTnnXNxc8Ho+HuehnHN4eYGccz5cdyfI4/F4BAAAAPgh2J9PAgAAAAyGSQAAAPiNYRIAAAB+Y5gEAACA3xgmAQAA4DeGSQAAAPiNYRIAAAB+Y5gEAACA3xgmAQAA4DeGSQBAtYKCgnTvvfc29mIAaMIYJgGgCfrPf/5jDXLm8qtf/eqi9/n+979v3X7JJZc0+PIBwHmtKv8GAPVo//79GjRokEJDQy96e1lZmd555x2dPXu2Rd2vb9++qkmbNm20YsUKpaene11/5swZbdiwwbodABoTayYBNAiPx6Nhw4bp9OnTF70MHjzYuk9Lu19tbrrpJh04cEDZ2dle15tB0gyk3/zmN+vxfw0AascwCQBN2DXXXKPevXvrz3/+s9f1y5cv1w033KDLLrvsgs8xg+a3vvUtdevWTS6Xy1r7+fDDD6u8vNzrfu+9954mTJigLl26WGs4r7zySn3ve99TUVFRjctkNrsHBwfr//7v/2z6KgE4GZu5AaCJmzRpkpYtW6ZHH33U2kfy5MmT2rp1q5YuXaqXXnrpgvs/++yz1n6UP/vZz6w/X331Vc2ePVvFxcX6zW9+Y93HrNUcO3asSktLdd9991kD5X//+19t2rRJp06dUseOHS+6LGZz+69//WstWLBAd911V71/7QCaPoZJAGjibrvtNmuA+/vf/65rr71Wq1atstYk3nzzzRcdJs1azLZt21Z+fM8991iXefPmWWsVzdpKs+n8yJEjWr16tb7zne9U3tcMndWZOXOm5s6dq8WLF+uOO+6oh68UgBOxmRsAmrj+/fsrLi7OOhDn/LB4yy23qF27dhe9f9VB8pNPPrHWZI4YMUKffvqp3n33Xev682set2zZYl1fE7Nvpzk90O9//3trDSmDJICqGCYBwCFrJ81axMOHD+uNN96wPq7pyPlbb73VGhg7dOigzp076wc/+IF12/n9Ic1+mGYz+MKFCxUREWFt8v7DH/5w0f0ln3vuOes2s4+k2eQOAFUxTAKAA5ghzqxhNPsphoeHa8yYMRe9n9nfceTIkdbR3w899JBeeOEFbdu2TY899ph1e0VFReV9f/vb3yonJ0e/+MUvVFJSop/85CfWWtAPPvjAqzl8+HBdccUVevrpp/Xxxx/X81cKwGkYJgHAAXr06GENdTt27FBSUpJatbr4Lu/m9oKCAusgnJ/+9Kf69re/rdGjRyssLOyi9x8wYIB1UM3rr7+unTt3WgfhzJ8/3+s+kZGR1gE/x44ds44gN5vOAeA8hkkAcAhz8MycOXOso6+rExISYv1Z9RyW5shtc/BNVebI7s8+++yCwdKc8scc4f1lZp/NzZs3WydaHzdunLUmEwAMjuYGAIcwm6/NpSYJCQnWWkhzkIzZbG1OJWROIfTlE6Sb0wWZg2rMWs6oqChrsDT3M8OoOffkxVx99dXWOSzNidTNEeDr169X69atbf0aATgPwyQANCNmf0pzrsgZM2ZYm6/NYGkOvrn++uutg2zOi4+Ptz42+1SaTdvmyHBz3V//+ldraKzOqFGjrFMTmYHz9ttvt44sN2szAbRcDJMA0AT16tWrTm+3aPaNNJcvr53Mysq64L5Ve+Zo7kWLFtXav9gymPNbnjt3rtbPBdAy8OskAAAA/MaaSQAN5h//+Ic6dep00dtOnz7dYu8HAE4W5KnLdhQAAADgItjMDQAAAL8xTAIAAMBvDJMAAADwG8MkAAAAmsPR3IcaewGAJqdtjzlykpK8jMZeBACAraJqvQdrJgEAAOA3hkkAAAD4jWESAAAAfmOYBAAAQMsYJpcvf1GjRiVrwIBEJSXNUE6OPQft0KXr1O7wYdF6PnOmcnfNU0neCo0bM9SWZa2vrtO+v3Tp0qVLtxkNk5s375TbvVDTpk3SunVPKjq6t5KTZ6ug4BRdui22276dS28fyNP09MyAOg3Vddr3ly5dunTpNqNhcvHi9Zo4cawmTBityMgeysiYqjZtXFqzZhtdui22u3VHtjKeWKWNW3YH1GmortO+v3Tp0qVLt5kMk2Vl57R//2ElJMRXXhccHKyEhIHat+8gXbotsus0Tvv+0qVLly7dehomT548qccff1y33nqrrrnmGuti/v6b3/xG+fn5qg+FhcUqL69QeHiY1/Xh4Z108mQhXbotsus0Tvv+0qVLly7dehgmd+3apaioKD311FPq2LGjrrvuOuti/m6ui46O1u7dtW8WKy0tVXFxsdeltLTMl0UBAACA095O8b777lNSUpLmz5+voKAgr9s8Ho/uuece6z5ZWVk1dtxutzIyvN92bc6ce/Xgg/dd9P5hYR0UEhKsggLv6dnsNBoR4T1l+4IuXSd3ncZp31+6dOnSpVsPayazs7N1//33XzBIGuY6c9tbb71VayctLU1FRUVel7S0u6u9f2hoa/XvH6msrJzK6yoqKpSVla1Bg67y5UugS7fZdJ3Gad9funTp0qVbD2smu3TpojfffNPanH0x5rYrrrii1o7L5bIu3kJr/JwpU8YrNXWuYmMjFRcXpSVLNqik5KwSE0f78iXQpdusuuYUPn17dan8uFf3zorr11OFp07r6LGCJtd12veXLl26dOnaPEzOnDlTKSkp2rNnj66//vrKwfH48eN65ZVX9Kc//UlPPPGE6sNNN43Qxx8X6amnlis/v1AxMX20cGFGwKtn6dJ1cndwXB9tXTW78uPH50y2/ly6+jWlzJjf5LpO+/7SpUuXLt3aBXnMzo4++Mtf/qK5c+daA2V5ebl1XUhIiIYMGaKf/exnmjhxovxjz1nYgeakbY85cpKSPO99oQEAThdl/zB53rlz56zTBBkRERFq3bq1AsMwCXwZwyQAoKkPkz5t5q7KDI9du3b199MBAADQDDjiHXAAAADQNDFMAgAAwG8MkwAAAPAbwyQAAAD85vfR3PbjaG44l9OOusbnOPocAAI/mps1kwAAAPAbwyQAAAD8xjAJAAAAvzFMAgAAoGUMk8uXv6hRo5I1YECikpJmKCfHnoN26NKt7+7wYdF6PnOmcnfNU0neCo0bM9SWZaVbv12n/ZzRpUuXbmN0HTNMbt68U273Qk2bNknr1j2p6OjeSk6erYKCU3TpNvlu+3YuvX0gT9PTMwPq0G3YrtN+zujSpUu3obuOGiYXL16viRPHasKE0YqM7KGMjKlq08alNWu20aXb5Ltbd2Qr44lV2rhld0Adug3bddrPGV26dOk2dNcxw2RZ2Tnt339YCQnxldcFBwcrIWGg9u07SJduk+7CmZz2c0aXLl26jfX65ohhsrCwWOXlFQoPD/O6Pjy8k06eLKRLt0l34UxO+zmjS5cu3cZ6fbN9mDx69Kh++MMf1nif0tJSFRcXe11KS8vsXhQAAADUM9uHyY8//lhLliyp8T5ut1sdO3b0urjdC6q9f1hYB4WEBKugwHt6NjuNRkR4T9m+oEu3IbpwJqf9nNGlS5duY72++TxMbty4scbL9u3ba22kpaWpqKjI65KWdne19w8Nba3+/SOVlZVTeV1FRYWysrI1aNBVvn4JdOk2aBfO5LSfM7p06dJtrNe3Vr5+wvjx4xUUFCSPx1PtfcztNXG5XNbFW2iNnzNlynilps5VbGyk4uKitGTJBpWUnFVi4miflp8u3cbomlPX9O3VpfLjXt07K65fTxWeOq2jxwroNtGu037O6NKlS7ehu34Nk127dtW8efN0yy23XPT2t956S0OGDJHdbrpphD7+uEhPPbVc+fmFionpo4ULMwJePUuXbkN0B8f10dZVsys/fnzOZOvPpatfU8qM+XSbaNdpP2d06dKl29BdI8hT0yrGi7j55ps1cOBAPfTQQxe9PTvbrDIdZK0+9Y09Z2EHGkPbHnMaexHgh5K8jMZeBABo4qLsXzM5a9YsnTlzptrbIyMj67TfJAAAAJzP52FyxIgRNd7evn17jRw5MpBlAgAAgEM44qTlAAAAaJoYJgEAAOA3hkkAAAD4jWESAAAADXcADuBknMIHAAB7sWYSAAAAfmOYBAAAgN8YJgEAAOA3hkkAAAC0jGFy+fIXNWpUsgYMSFRS0gzl5Njzft506Z43fFi0ns+cqdxd81SSt0Ljxgy1ZVnpOrPrtJ9funTp0m2MrmOGyc2bd8rtXqhp0yZp3bonFR3dW8nJs1VQcIouXdu67du59PaBPE1PzwyoQ7d5dJ3280uXLl26Dd111DC5ePF6TZw4VhMmjFZkZA9lZExVmzYurVmzjS5d27pbd2Qr44lV2rhld0Adus2j67SfX7p06dJt6K5jhsmysnPav/+wEhLiK68LDg5WQsJA7dt3kC5dW7qAk39+6dKlS7exXjcdMUwWFharvLxC4eFhXteHh3fSyZOFdOna0gWc/PNLly5duo31uunzMFlSUqK//e1vOnDgwAW3nT17Vs8991ytjdLSUhUXF3tdSkvLfF0UAAAANDKfhslDhw4pJiZG1113nQYMGKCRI0fqww8/rLy9qKhIU6ZMqbXjdrvVsWNHr4vbvaDa+4eFdVBISLAKCrynZ7PTaESE95TtC7p0geby80uXLl26jfW66dMwmZqaqtjYWJ04cUIHDx7UpZdequHDhysvL8+nfzQtLc0aPKte0tLurvb+oaGt1b9/pLKyciqvq6ioUFZWtgYNusqnf5suXaA5/vzSpUuXbmO9brby5c5vvPGGXn75ZUVERFiXF154QVOnTtWIESO0fft2tW/fvk4dl8tlXbyF1vg5U6aMV2rqXMXGRiouLkpLlmxQSclZJSaO9uVLoEu31lPM9O3VpfLjXt07K65fTxWeOq2jxwrotrCu035+6dKlS7ehuz4Pk2Z/yVatvviUoKAgPfPMM7r33nutTd5//vOfVV9uummEPv64SE89tVz5+YWKiemjhQszAl49S5duVYPj+mjrqtmVHz8+Z7L159LVryllxny6LazrtJ9funTp0m3orhHk8Xg8db3zsGHDdN999+n222+/4DYzUC5fvtw6mKa8vNyPRbHnLOxATdr2mNPYi4AmpCQvo7EXAQCauCh795m89dZbtWLFiove9vTTT2vSpEnyYTYFAACAw/m0ZrJ+sWYS9Y81k6iKNZMA0MBrJgEAAICqGCYBAADgN4ZJAAAA+I1hEgAAAA1znkmgoXCgDAAAzsCaSQAAAPiNYRIAAAB+Y5gEAACA3xgmAQAA0DKGyeXLX9SoUckaMCBRSUkzlJNjz7vm0HVed/iwaD2fOVO5u+apJG+Fxo0Zasuy0qXr5McFXbp06TZG1zHD5ObNO+V2L9S0aZO0bt2Tio7ureTk2SooOEW3BXbbt3Pp7QN5mp6eGVCHLt3m9LigS5cu3YbuOmqYXLx4vSZOHKsJE0YrMrKHMjKmqk0bl9as2Ua3BXa37shWxhOrtHHL7oA6dOk2p8cFXbp06TZ01zHDZFnZOe3ff1gJCfGV1wUHByshYaD27TtIt4V1gYbgtMcFXbp06TbW67EjhsnCwmKVl1coPDzM6/rw8E46ebKQbgvrAg3BaY8LunTp0m2s12Of3wHnnXfe0T/+8Q9dc801io6O1rvvvqvf//73Ki0t1Q9+8AONGjWq1oa5r7lU5XKVyeUK9XVxAAAA0Ih8WjP50ksvaeDAgZo5c6YGDRpkfXzdddfp8OHDev/99zVmzBi9+uqrtXbcbrc6duzodXG7F1R7/7CwDgoJCVZBgff0bHYajYjwnrJ9QdeZXaAhOO1xQZcuXbqN9Xrs0zD50EMPadasWSooKNDixYt122236a677tK2bdv0yiuvWLc9+uijtXbS0tJUVFTkdUlLu7va+4eGtlb//pHKysqpvK6iokJZWdkaNOgqX74Eus2gCzQEpz0u6NKlS7exXo992sy9f/9+Pffcc9bfJ06cqNtvv13f+c53Km///ve/bw2ZtXG5XNbFW82buKdMGa/U1LmKjY1UXFyUlizZoJKSs0pMHO3Ll0C3mXTNqWD69upS+XGv7p0V16+nCk+d1tFjBXTp2tJ12uOCLl26dBu669c+k0FBQZVHAbVp08baRH3epZdeaq1lrA833TRCH39cpKeeWq78/ELFxPTRwoUZAa+epevM7uC4Ptq6anblx4/PmWz9uXT1a0qZMZ8uXVu6Tntc0KVLl25Dd40gj8fjqeud4+Pj9dhjj+mGG26wPv7Xv/5lHYTTqtXnM+nOnTt1xx13KDc3149Fsecs7Gge2vaY09iLgBagJC+jsRcBAJq4KHvXTP74xz9WeXl55cexsbFet//1r3+t09HcAAAAaB58WjNZv1gziS+wZhINgTWTABD4mklHnLQcAAAATRPDJAAAAPzGMAkAAAC/MUwCAACg4c4zCWfigBYAAFAfWDMJAAAAvzFMAgAAwG8MkwAAAPAbwyQAAABaxjC5fPmLGjUqWQMGJCopaYZycux51xy60vBh0Xo+c6Zyd81TSd4KjRsz1JZlpUvXyV2nPY7p0qVLtzG6jhkmN2/eKbd7oaZNm6R1655UdHRvJSfPVkHBKbo2dNu3c+ntA3manp4ZUIcu3ebUddrjmC5dunQbuuuoYXLx4vWaOHGsJkwYrcjIHsrImKo2bVxas2YbXRu6W3dkK+OJVdq4ZXdAHbp0m1PXaY9junTp0m3orm3DpMfjUX0qKzun/fsPKyEhvvK64OBgJSQM1L59B+kG2AXg/McxXbp06TbW/GDLMOlyufTOO++ovhQWFqu8vELh4WFe14eHd9LJk4V0A+wCcP7jmC5dunQba37w6R1wfvazn130+vLycj366KMKDw+3Pv7d735XY6e0tNS6VOVylcnlCvVlcQAAANDIfBomn3zyScXHx6tTp04XbOY2aybbt2+voKCgWjtut1sZGRle182Zc68efPC+i94/LKyDQkKCVVDgPT2bnUYjIrynbF/QBdBcHsd06dKl21jzg0+buX/961+rqKhIv/zlL7V9+/bKS0hIiJ599lnr76+++mqtnbS0NKtT9ZKWdne19w8Nba3+/SOVlZVTeV1FRYWysrI1aNBVvnwJdAE0y8cxXbp06TbW/ODTmsmf//znuv766/WDH/xA48aNs9Ywtm7d2q99LM3FW82buKdMGa/U1LmKjY1UXFyUlizZoJKSs0pMHO3zv0/34qdW6durS+XHvbp3Vly/nio8dVpHjxXQpdsiu057HNOlS5duQ3eNII8fh2KfPn1a06ZN01tvvaXly5dr8ODB1t/79esXwKLUfuLMZcs2adGitcrPL1RMTB+lp6coPj7wiboldNv2mFPj7SOujtHWVbMvuH7p6teUMmO+38tKl25T7pbkee9u09Qfx3Tp0qXb8N2o+hkmz1u5cqWmT5+u/Px8vf322/U+TMJ/tQ2TQEtUl2ESAFq2KHs3c3/Z9773PV177bXas2ePevbsGUgKAAAADhTQMGlceeWV1gUAAAAtj2PeThEAAABND8MkAAAA/MYwCQAAAL8xTAIAAMBvAZ0ayF6cGsjgFD6A83HKIQAt6dRArJkEAACA3xgmAQAA4DeGSQAAAPiNYRIAAAAtY5hcvvxFjRqVrAEDEpWUNEM5OYdaXHf4sGg9nzlTubvmqSRvhcaNGWrLstKlS7fhuk573qFLly7dZjFMbt68U273Qk2bNknr1j2p6OjeSk6erYKCUy2q276dS28fyNP09MyAOnTp0m28rtOed+jSpUu3WQyTixev18SJYzVhwmhFRvZQRsZUtWnj0po121pUd+uObGU8sUobt+wOqEOXLt3G6zrteYcuXbp0HT9MlpWd0/79h5WQEF95XXBwsBISBmrfvoMtpgvA+Zz2vEOXLl26zWKYLCwsVnl5hcLDw7yuDw/vpJMnC1tMF4DzOe15hy5dunRr00oBOHPmjFatWqXDhw+ra9eumjRpksLDw2v9vNLSUutSlctVJpcrNJDFAQAAQAPzac1kv3799PHHH1t/P3r0qGJjY3X//fdr27ZtmjNnjnX7kSNHau243W517NjR6+J2L6j2/mFhHRQSEqyCAu/p2ew0GhHhPWX7wmldAM7ntOcdunTp0rV1mHz33Xf12WefWX9PS0tTt27d9P777+vNN9+0/oyLi9MDDzxQa8d8blFRkdclLe3uau8fGtpa/ftHKisrp/K6iooKZWVla9Cgq3z5EhzdBeB8TnveoUuXLt1628ydlZWl+fPnW2sVjUsuuUQZGRn63ve+V+vnulwu6+Kt5k3cU6aMV2rqXMXGRiouLkpLlmxQSclZJSaO9vdLcGTXnKqkb68ulR/36t5Zcf16qvDUaR09VkCXLl0HdJ32vEOXLl26NQnyeDwe1ZE58uf48ePq3LmzvvKVr2jLli3Wpu7zzNrJ6OholZSUyHe1nzhz2bJNWrRorfLzCxUT00fp6SmKjw98om5K3bY95tR4+4irY7R11ewLrl+6+jWlzJjv97LSpUvXvm5JXoajnnfo0qVLt3pRsn2YNMNjq1at9N577+nZZ5/VhAkTKm9//fXXddttt+mDDz6Q7+w5C7vT1TZMAmj66jJMAoAzRNm7mdscZFOV2bRd1QsvvKARI0b4kgQAAICD+bRmsn6xZtJgzSTgfKyZBNCS1kw64qTlAAAAaJoYJgEAAOA3hkkAAAD4jWESAAAAjfPe3C0ZB8oAAACwZhIAAAABYJgEAACA3xgmAQAA4DeGSQAAALSMYXL58hc1alSyBgxIVFLSDOXkHGqy3eHDovV85kzl7pqnkrwVGjdmqC3LSpcuXed3nfZ8RpcuXbrNYpjcvHmn3O6FmjZtktate1LR0b2VnDxbBQWnmmS3fTuX3j6Qp+npmQF16NKl2/y6Tns+o0uXLt1mMUwuXrxeEyeO1YQJoxUZ2UMZGVPVpo1La9Zsa5LdrTuylfHEKm3csjugDl26dJtf12nPZ3Tp0qXr+GGyrOyc9u8/rISE+MrrgoODlZAwUPv2HWxyXQBoLs9ndOnSpdsshsnCwmKVl1coPDzM6/rw8E46ebKwyXUBoLk8n9GlS5eurcPk3r17deTIkcqPly5dquHDh6t79+669tprtXLlyjp1SktLVVxc7HUpLS3zZVEAAADQBPg0TE6ZMkX//ve/rb8vXLhQd999t4YOHaoHHnhA//M//6O77rpLmZm176judrvVsWNHr4vbvaDa+4eFdVBISLAKCrynZ7PTaESE95Tti/rqAkBzeT6jS5cuXVuHyffee09f/epXrb/PmzdPv//9763LPffco7lz52rBggX67W9/W2snLS1NRUVFXpe0tLurvX9oaGv17x+prKycyusqKiqUlZWtQYOu8uVLaJAuADSX5zO6dOnSrU0r+aBdu3Y6efKkevbsqf/+978aNmyY1+1f+9rXvDaDV8flclkXb6E1fs6UKeOVmjpXsbGRiouL0pIlG1RSclaJiaN9+RIarGtOKdK3V5fKj3t176y4fj1VeOq0jh4roEuXbgvuOu35jC5dunRrEuTxeDyqo9tvv90aAs0m7okTJ+qqq67Sww8/7LX5esWKFcrJ+WLyrbvaT5y5bNkmLVq0Vvn5hYqJ6aP09BTFxwc+UfvTbdtjTo23j7g6RltXzb7g+qWrX1PKjPl+LytdunSbfrckL8NRz2d06dKlW70o2TpMHjt2zDrgpkePHta+ks8884yGDBmimJgYHTx4UP/4xz+0bt063XTTTfKdPWdhbyi1DZMAWq66DJMA4AxR9u4z2a1bN+3bt0/XXHONXnrpJZk59M0339TWrVt15ZVX6u9//7ufgyQAAACcyKc1k/WLNZMAmgfWTAJoPmxeMwkAAABUxTAJAAAAvzFMAgAAwG8MkwAAAPBbsz8AhwNlADQXHNgDoOFxAA4AAADqEcMkAAAA/MYwCQAAAL8xTAIAAKBlDJPLl7+oUaOSNWBAopKSZignJ/CDdoYPi9bzmTOVu2ueSvJWaNyYobYsK126dOk2dLe+nifp0qVLt1kMk5s375TbvVDTpk3SunVPKjq6t5KTZ6ug4FRA3fbtXHr7QJ6mp2fatqx06dKl2xjd+nqepEuXLt2atJJDLF68XhMnjtWECaOtjzMypmrHjl1as2abUlKS/O5u3ZFtXexGly5dug3dra/nSbp06dJ1/JrJsrJz2r//sBIS4iuvCw4OVkLCQO3bd7BRlw0AmvPzJF26dOk2i2GysLBY5eUVCg8P87o+PLyTTp4sbLTlAoDm/jxJly5durYOk/fdd5927typQJWWlqq4uNjrUlpaFnAXAAAADcunYfIPf/iDvv71rysqKkqPPfaYPvroI7/+UbfbrY4dO3pd3O4F1d4/LKyDQkKCVVDgPT2bnUYjIrynbABoierreZIuXbp0bd/MvXXrVt1000164okn1KNHD91yyy3atGmTKioq6txIS0tTUVGR1yUt7e5q7x8a2lr9+0cqKyun8jrz72VlZWvQoKt8/RIAoNmpr+dJunTp0rX9aO4BAwbo+uuv129+8xutW7dOmZmZGj9+vK644grdeeedmjJliiIjI2tsuFwu6+IttMbPmTJlvFJT5yo2NlJxcVFasmSDSkrOKjHx86OSAjlFR99eXSo/7tW9s+L69VThqdM6eqyALl26dB3Tra/nSbp06dKtSZDH4/GojsyRP2bT9uWXX+51fV5enjVUPvvsszp69KjKy8vlu9pPnLls2SYtWrRW+fmFionpo/T0FMXH1zxRt+0xp8bbR1wdo62rZl9w/dLVryllxvw6LDddunTpNky3JC+jXp4n64IuXbottRvVMMPkeSb18ssv65vf/KZ8Z89Z2H0dJgHAKeoyTAKAvaLs3WeyZ8+eCgkJqfb2oKAgPwdJAAAAOJFP+0weOXKk/pYEAAAAjuOIk5YDAACgaWKYBAAAgN8YJgEAAOA3hkkAAAD4zadTA9Wntj0mNfYiAECLxCmHADTYqYEAAACAqhgmAQAA4DeGSQAAAPiNYRIAAADNf5gcPixaz2fOVO6ueSrJW6FxY4bSpUuXLt0G6BrLl7+oUaOSNWBAopKSZign5xBdunTpOmuYbN/OpbcP5Gl6eiZdunTp0m3A7ubNO+V2L9S0aZO0bt2Tio7ureTk2SooOEWXLt1pLbvr83tzN6atO7KtC126dOnSbdju4sXrNXHiWE2YMNr6OCNjqnbs2KU1a7YpJSWJLl26arldR62ZBAA0vLKyc9q//7ASEuIrrwsODlZCwkDt23eQLl26Lbhb2Qq4AABotgoLi1VeXqHw8DCv68PDO+nkyUK6dOm24K7fw+TTTz+tyZMna+XKldbHS5cuVb9+/RQdHa1f/OIX+uyzz2ptlJaWqri42Ovi8ZT79xUAAACg0fg0TP7qV7+yBsZPP/1U999/vx577DHrz+9///u64447tHDhQj388MO1dtxutzp27Oh1+az4QCBfBwCgHoSFdVBISLAKCrzXXpid9iMivNdy0KVL91SL6vo1TD777LPW5fnnn9dLL72kBx54QL///e+tP9PS0rRgwQL9+c9/rrVj7ltUVOR1adWhXyBfBwCgHoSGtlb//pHKysqpvK6iokJZWdkaNOgqunTptuCuX0dzHzt2TEOHfn7esvj4eGvnzYEDB1bePnjwYOs+tXG5XNalqqCgkFpPedG3V5fKj3t176y4fj1VeOq0jh4r8OXLoEuXLl26PpgyZbxSU+cqNjZScXFRWrJkg0pKziox8fOjQunSpRvVYrtGkMfj8dT1zn369NG8efN0ww036L333rP2kzT7TiYlfX5I+ebNmzVt2jQdOXLE5wVp22NSjbePuDpGW1fNvuD6patfU8qM+T7/e3Tp0qVL93MleRm1tpct26RFi9YqP79QMTF9lJ6eovj4wNdo0KVLt6l3o+wdJn/5y19am7JvueUWvfLKK/rud79rbdY2m62DgoL0yCOP6Dvf+Y5+97vfye5hEgBQP+oyTAJoqaLs3cydkZGhtm3bKisrS3fddZd+/vOfW5u7//d//9c6KGfcuHF1OgAHAAAAzYNPaybrE2smAaBxsGYSQCBrJjlpOQAAAPzGMAkAAAC/MUwCAADAbwyTAAAA8BvDJAAAAPzGMAkAAAC/MUwCAADAbwyTAAAA8BvDJAAAAJr/MDl8WLSez5yp3F3zVJK3QuPGDKVLly5dug3QNZYvf1GjRiVrwIBEJSXNUE7OIbp06dK1OGaYbN/OpbcP5Gl6eiZdunTp0m3A7ubNO+V2L9S0aZO0bt2Tio7ureTk2SooOEWXLt1pLbtrtJJDbN2RbV3o0qVLl27DdhcvXq+JE8dqwoTR1scZGVO1Y8curVmzTSkpSXTp0lXL7TpqzSQAoOGVlZ3T/v2HlZAQX3ldcHCwEhIGat++g3Tp0m3B3cqWr5/w4Ycfavbs2Ro1apRiYmLUv39/jRs3TosWLVJ5eXnACwQAaDoKC4tVXl6h8PAwr+vDwzvp5MlCunTptuCuX8Pk7t27rQFy8+bNOnfunN577z0NGTJE7du318yZM3Xdddfpk08+qbVTWlqq4uJir4vHwyAKAADgND4Nk9OnT9f9999vDZU7d+7Us88+q0OHDmnlypXKzc3Vp59+qvT09Fo7brdbHTt29Lp8VnwgkK8DAFAPwsI6KCQkWAUF3msvzE77ERHeazno0qV7qkV1/Rom9+7dq9tvv73y49tuu8267vjx4woLC9Pjjz+u559/vtZOWlqaioqKvC6tOvTz7ysAANSb0NDW6t8/UllZOZXXVVRUKCsrW4MGXUWXLt0W3PXraO7LL7/c2meyT58+1sdmiPzss8/UoUMH6+OvfvWr+vjjj2vtuFwu61JVUFBIrae86NurS+XHvbp3Vly/nio8dVpHjxX48mXQpUuXLl0fTJkyXqmpcxUbG6m4uCgtWbJBJSVnlZj4+VGhdOnSjWqxXSPI4/F4fNnM/corr+g3v/mNNQw+/PDDMp++fft26/YtW7Zo2rRpOnz4sM8L0rbHpBpvH3F1jLaumn3B9UtXv6aUGfN9/vfo0qVLl+7nSvIyam0vW7ZJixatVX5+oWJi+ig9PUXx8YGv0aBLl25T70bZO0yePn1aycnJWrt2rXXk9jXXXKNly5apd+/e1u1bt261NlknJSXZPkwCAOpHXYZJAC1VlL3D5Hlnz561Nm9fcsklsgvDJAA0DoZJAIEMk369A06bNm38+TQAAAA0M7wDDgAAAPzGMAkAAAC/MUwCAADAbwyTAAAA8JtfB+AAABoeR10DaIpYMwkAAAC/MUwCAADAbwyTAAAA8BvDJAAAAJr/MDl8WLSez5yp3F3zVJK3QuPGDKVLly5dul+yfPmLGjUqWQMGJCopaYZycg7RpUuXbr12/Romy8rKtGrVKt1///2aNGmSdTF/X716tXVbfWjfzqW3D+RpenomXbp06dK9iM2bd8rtXqhp0yZp3bonFR3dW8nJs1VQcIouXbp0VR9dv04NdPjwYY0dO1bHjh3T1772NV1xxRXW9fv27dP8+fN15ZVX6q9//asiIyNlp607sq2L3ejSpUu3uXQXL16viRPHasKE0dbHGRlTtWPHLq1Zs00pKUl06dKlK7u7fq2Z/PGPf6wBAwbo+PHj2rFjh/7yl79YF/N3c13//v01bdq0gBYKAOCbsrJz2r//sBIS4iuvCw4OVkLCQO3bd5AuXbp0ZXe3suXrJ/z973/Xr371K3Xo0OGC28x1Dz/8sHbu3BnwggEA6q6wsFjl5RUKDw/zuj48vJNOniykS5cuXdnd9Xszd6dOnfSf//xHsbGxF73d3GbuU5PS0lLrUpXHU66goBBfFwcAAACNyOc1kz/60Y80efJkzZ07Vzk5OdambXMxfzfX3XnnnUpJSamx4Xa71bFjR6/LZ8UHAvk6AKBFCwvroJCQYBUUeK9lMDvXR0R4r42gS5cuXTu6fg+TDz30kFJTU/Wb3/xGAwcOVLdu3ayL+bu5ztz24IMP1thIS0tTUVGR16VVh36BfB0A0KKFhrZW//6RysrKqbyuoqJCWVnZGjToKrp06dKV3V2/N3MbZmA0lyNHjuijjz6yruvSpYt69+5dp893uVzWparaNnGbU2n07dWl8uNe3Tsrrl9PFZ46raPHCvz5MujSpUu3WXWnTBmv1NS5io2NVFxclJYs2aCSkrNKTPz86E26dOnSjbW5awR5PB6PbHT06FHNmTNHmZm+nT+tbY9JNd4+4uoYbV01+4Lrl65+TSkz5vu8nHTp0qXrtG5JXkat7WXLNmnRorXKzy9UTEwfpaenKD4+8DUPdOnSbandqIYfJrOzszV48GCVl5fbOkwCQEtXl2ESAOwVZf9m7o0bN9Z4e25urq9JAAAAOJTPw+T48eMVFBSkmlZomtsBAADQ/Pl8NHfXrl21du1a6yigi1327t1bP0sKAAAA5w+TQ4YM0Z49e6q9vba1lgAAAGjBm7lnzZqlM2fOVHt7ZGSktm/fHuhyAQAAwAFsP5rbXxzNDQA142huAM3iaG4AQONo22OOnIThF2gZfN5nEgAAADiPYRIAAAB+Y5gEAACA3xgmAQAA0PyHyeHDovV85kzl7pqnkrwVGjdmKF26dOnSdXDXWL78RY0alawBAxKVlDRDOTmH6NKlu9xZXduHyePHj+uhhx6yO6v27Vx6+0Cepqdn0qVLly7dZtDdvHmn3O6FmjZtktate1LR0b2VnDxbBQWn6NKlO80Z3Xo5NdBHH32kjIwMzZ4929bu1h3Z1sVudOnSpUu3cbqLF6/XxIljNWHCaOvjjIyp2rFjl9as2aaUlCS6dOmq6Xf9WjOZk5NT4+XgwYMBLRAAoPkrKzun/fsPKyEhvvK64OBgJSQM1L59/r+O0KVLt+G6fq+ZHDhwYLXvv33+evMnAADVKSwsVnl5hcLDw7yuDw/vpNzcD+jSpeuArt/D5GWXXabHH39c119//UVv379/v8aNG1djo7S01LpU5fGUKygoxNfFAQAAQCPyeZgcMmSIjh07pp49e1709lOnTl10rWVVbrfb2q+yqpAO/dW64wBfFwcA4EBhYR0UEhKsgoJCr+vNwQAREd5rT+jSpXuqSXb93mfynnvuUa9evaq9vUePHlq8eHGNjbS0NBUVFXldWnXo5+uiAAAcKjS0tfr3j1RWVk7ldRUVFcrKytagQVfRpUvXAV2/10zeeuutNd4eFhamO+64o8b7uFwu61JVbZu4zakp+vbqUvlxr+6dFdevpwpPndbRYwV1Wna6dOnSpdt0ulOmjFdq6lzFxkYqLi5KS5ZsUEnJWSUmfn60KV26dKOafNcI8tS2TdpHR48e1Zw5c5SZ6dv5yNr2mFTj7SOujtHWVReebmjp6teUMmO+z8tJly5dunTrt1uS570708UsW7ZJixatVX5+oWJi+ig9PUXx8YGvKaFLl65d3aiGHyazs7M1ePBglZeX2zpMAgCcpS7DJICmLsr+zdwbN26s8fbc3FxfkwAAAHAon4fJ8ePHV3ueyfM4zyQAAEDL4PPR3F27dtXatWuto4Audtm7d2/9LCkAAACcP0ya80zu2bOn2ttrW2sJAACAFryZe9asWTpz5ky1t0dGRmr79u2BLhcAAAAcwPajuf3F0dwA0LxwNDfQHNTD0dwAANRF2x5z6qXLkAo4fJ9JAAAA4DyGSQAAAPiNYRIAAAB+Y5gEAABA8x8mhw+L1vOZM5W7a55K8lZo3JihdOnSpUuX7kUtX/6iRo1K1oABiUpKmqGcnEN06dJdXj9dv4fJDz74QKdPn77g+nPnzun111+X3dq3c+ntA3manp5Jly5dunTpVmvz5p1yuxdq2rRJWrfuSUVH91Zy8mwVFJyiS5fuNHu7fp0a6MMPP9Qtt9xivQuOebeb2267TfPmzdMll1xi3f7xxx/rG9/4hsrLy2WnrTuyrYvd6NKlS5du8+ouXrxeEyeO1YQJo62PMzKmaseOXVqzZptSUpLo0qUr+7p+rZn8+c9/ruDgYP3zn//USy+9pAMHDljDY2FhYeV9msh50AEALUxZ2Tnt339YCQnxldeZ16yEhIHat+8gXbp0bexWtnz9hJdffllPPfWUhg4dqtGjR+vvf/+7unbtqlGjRllrJQ2zxhIAgIZWWFis8vIKhYeHeV0fHt5JJ09+sdKDLl26Crjr9zBZVFSksLAvFsblcmnt2rXq1auXtYbyxIkTtTZKS0tVXFzsdfF47N0sDgAAgPrn8zDZp08f5eTkeF3XqlUrrV692rrt29/+dq0Nt9utjh07el0+Kz7g66IAAOAlLKyDQkKCVVDgvbbFHGQQEeG9VoYuXbqnAur6PUzeeOON+uMf/3jB9ecHyoEDB9a6z2RaWpq1hrPqpVWHfr4uCgAAXkJDW6t//0hlZX2x0qOiokJZWdkaNOgqunTp2tj1+2juRx55RJ9++unFY61aac2aNfrvf/9bY8NsGjeXqoKCQmo9hUTfXl0qP+7VvbPi+vVU4anTOnqswKevgS5dunTpNt/ulCnjlZo6V7GxkYqLi9KSJRtUUnJWiYmfH8VKly7dKNu6RpDH5kOvjx49qjlz5igz07fzhrXtManG20dcHaOtq2ZfcP3S1a8pZcZ8n5eTLl26dOk6s1uSl1Fre9myTVq0aK3y8wsVE9NH6ekpio8PfA0MXbotrxvV8MNkdna2Bg8e7PN5JmsbJgEAqOswCcAuUfZv5t64cWONt+fm5vqaBAAAgEP5PEyOHz/eOo9kTSs0Oc8kAABAy+Dz0dzmBOXmvJLmKKCLXfbu3Vs/SwoAAADnD5NDhgyx3pe7OrWttQQAAEAL3sw9a9YsnTlzptrbIyMjtX379kCXCwAAAA5g+9Hc/uJobgBAXXA0N+Dwo7nrC08On2vbY05jLwIAAED97TMJAAAAnMcwCQAAAL8xTAIAAMBvDJMAAADwm6OGyeXLX9SoUckaMCBRSUkzlJNzqMV1hw+L1vOZM5W7a55K8lZo3JihtiwrXbp06TaXrtOe1+nSdXrXr2GyoKDAOpfkxx9/bH188uRJPfbYY3rooYf0zjvvqD5s3rxTbvdCTZs2SevWPano6N5KTp6tgoJTLarbvp1Lbx/I0/T0zIA6dOnSpdtcu057XqdL18ldv04N9Oabb2rMmDEqLi5Wp06dtG3bNiUlJalVq1bW2yk++uij+tvf/qbBgwfLTosXr9fEiWM1YcJo6+OMjKnasWOX1qzZppSUpBbT3boj27rYjS5dunSbS9dpz+t06Tq569eayQceeMAaHouKivSLX/xC48eP1/XXX69Dhw7p8OHD+t73vqeHH35YdiorO6f9+w8rISH+iwUPDlZCwkDt23ewxXQBAM3reZ0uXSd3K1u+foJ5X+6f/exnuvTSS/XTn/5Ux44d01133VV5+7333qtdu3bJToWFxSovr1B4eJjX9eHhnXTyZGGL6QIAmtfzOl26Tu76vZm7rKxMbdu2tf7eunVrtWvXThEREZW3m7+bfSprUlpaal2qcrnK5HKF+ro4AAAAaEQ+r5ns3r27cnNzKz9euXKlunbtWvnxhx9+6DVcXozb7VbHjh29Lm73gmrvHxbWQSEhwSoo8J6ezU6jERHeU7YvnNYFADSv53W6dJ3c9XuYNPtEnjhxovLjb33rW5VrKo2NGzdq2LBhNTbS0tKsfS6rXtLS7q72/qGhrdW/f6SysnIqrzMH+2RlZWvQoKt8/RIc2wUANK/ndbp0ndz1ezP3nDlzaj1AJyQkpMb7uFwu6+Kt5k3cU6aMV2rqXMXGRiouLkpLlmxQSclZJSZ+flSSv5zWNafS6NurS+XHvbp3Vly/nio8dVpHj9W8ewFdunTptoSu057X6dJ1ctcI8ng8Htno6NGj1sCZmenrecNqP3HmsmWbtGjRWuXnFyompo/S01MUHx/4RN2Uum171Dysj7g6RltXzb7g+qWrX1PKjPl+LytdunTpOqVbkpfhqOd1unSd3Y1q+GEyOzvbOsdkeXm5j59pz1nYna62YRIAWrq6DJMA7BJl/2Zus09kTaoenAMAAIDmzedh0pykPCgoSDWt0DS3AwAAoPnz+WhucxqgtWvXWkcBXeyyd+/e+llSAAAAOH+YHDJkiPUuONWpba0lAAAAWvBm7lmzZunMmTPV3h4ZGant27cHulwAAABojsPkiBEjary9ffv2GjlyZCDLBAAAgOY6TMKZp7zglEMAAKBJ7DMJAAAAnMcwCQAAAL8xTAIAAMBvDJMAAABoGcPk8uUvatSoZA0YkKikpBnKybHn/bzpSsOHRev5zJnK3TVPJXkrNG7MUFuWlS5dunQbuuu051+6dJ3etW2Y7NOnj9577z3Vl82bd8rtXqhp0yZp3bonFR3dW8nJs1VQcIquDd327Vx6+0CepqdnBtShS5cu3cbuOu35ly5dJ3f9OjXQU089ddHr8/LytHjxYnXp0sX6+Cc/+YnstHjxek2cOFYTJoy2Ps7ImKodO3ZpzZptSklJohtgd+uObOtiN7p06dJt6K7Tnn/p0nVy169hcvr06frKV76iVq28P9W8L/dzzz2n1q1bW2+paOcwWVZ2Tvv3H9bdd3+n8rrg4GAlJAzUvn0H6QbYBYDmwmnPv3TpOrlb2fL1E1JSUhQREaHNmzfryJEjlZeQkBBt3brV+ntubq7sVFhYrPLyCoWHh3ldHx7eSSdPFtINsAsAzYXTnn/p0nVy1+9hcv78+Zo9e7bGjh2rp59+2q9/tLS0VMXFxV6X0tIyv1oAAABoPH4dgHPrrbcqKytL69at04033qiPPvrIp893u93q2LGj18XtXlDt/cPCOigkJFgFBd7Ts9lpNCLCe8r2BV0AaF6c9vxLl66TuwEfzW32m3z55Zd13XXXadCgQfJ4PHX+3LS0NBUVFXld0tLurvb+oaGt1b9/pLKycrz20czKytagQVf5+yXQBYBmxmnPv3TpOrnr9wE4VZkDbcxgOGbMGP3tb39T165d6/R5LpfLungLrfFzpkwZr9TUuYqNjVRcXJSWLNmgkpKzSkz8/Kgkf9H94hQdfXt9fiS+0at7Z8X166nCU6d19FgBXbp06Tqm67TnX7p0ndw1gjy+rFKsg6NHj2rOnDnKzPT1vGG1nzhz2bJNWrRorfLzCxUT00fp6SmKjw98om4J3bY95tR4+4irY7R11ewLrl+6+jWlzJjv97LSpUuXrt3dkrwMRz3/0qXr7G5Uww+T2dnZGjx4sMrLy338THvOwg7/hkkAcIq6DJMA7BJl/2bujRs31ni73acFAgAAQNPl8zA5fvx4a1/JmlZomtsBAADQ/Pl8NLc5yGbt2rXWUUAXu+zdu7d+lhQAAADOHyaHDBmiPXv2VHt7bWstAQAA0II3c8+aNUtnzpyp9vbIyEht37490OUCAABAcxwmR4wYUePt7du318iRIwNZJgAAADhEQCctBwCgOpzCB2gZ/H47RQAAAIBhEgAAAH5jmAQAAIDfGCYBAADQMobJ5ctf1KhRyRowIFFJSTOUk2PP+3nTlYYPi9bzmTOVu2ueSvJWaNyYobYsK126dOk2l+dJunTp1tMwaU5Qbs4r+ac//UmbNm3SuXPnVB82b94pt3uhpk2bpHXrnlR0dG8lJ89WQcEpujZ027dz6e0DeZqenhlQhy5dunSb6/MkXbp0bRomb7rpJhUVFVl///jjj3XNNdfo+uuv1wMPPKBbbrlFcXFxys/Pl90WL16viRPHasKE0YqM7KGMjKlq08alNWu20bWhu3VHtjKeWKWNW3YH1KFLly7d5vo8SZcuXZuGyZdeekmlpaXW39PT0/XJJ5/o3//+t06cOKH333/fOmn57NmzZaeysnPav/+wEhLiv1jw4GAlJAzUvn0H6QbYBYCG5rTnSbp06dbTZu5XX31VbrdbvXv3tj6+8sor9dhjj2nLli2yU2FhscrLKxQeHuZ1fXh4J508WUg3wC4ANDSnPU/SpUvX5nfACQoKsv4sLCxU3759L3hv7mPHjtX4+WbN5vm1m+e5XGVyuUL9WRwAAAA0Er/WTN55551KTEy0DrY5cuSI120fffSROnXqVOPnm7WZHTt29Lq43QuqvX9YWAeFhASroMB7ejY7jUZEeE/ZvqALAI3Dac+TdOnStXGYvOOOO3T55ZdbA6A54ObTTz/1un3NmjUaOHBgjY20tDTrIJ6ql7S0u6u9f2hoa/XvH6msrJzK6yoqKpSVla1Bg67y9UugCwCNzGnPk3Tp0rVxM/fixYtrvH3OnDkKCQmp8T4ul8u6eKt5E/eUKeOVmjpXsbGRiouL0pIlG1RSclaJiaPrvOx0az71R99eXSo/7tW9s+L69VThqdM6eqyALl26dG3vOu15ki5duhcX5DEnirTR0aNHrYEyM9PX85HVfuLMZcs2adGitcrPL1RMTB+lp6coPj7wiboldNv2mFPj7SOujtHWVRcehb909WtKmTHf72WlS5duy+2W5GU46nmSLl26FxOlBh8ms7OzNXjwYJWXl/v4mfachR3+DZMAYLe6DJMAmroo+zdzb9y4scbbc3NzfU0CAADAoXweJsePH2+dGqimFZrnTx0EAACA5s3no7m7du2qtWvXWkcBXeyyd+/e+llSAAAAOH+YHDJkiPbs2VPt7bWttQQAAEAL3sw9a9YsnTlzptrbzTvgbN++PdDlAgAAQHMcJkeMGFHj7e3bt9fIkSMDWSYAAAA057dTBAAAAAyGSQAAAPiNYRIAAAB+Y5gEAACA3xgmAQAA0DKGyeXLX9SoUckaMCBRSUkzlJNjz/t505WGD4vW85kzlbtrnkryVmjcmKG2LCtdunTpNpfnSbp06do0TH7wwQc6efJk5cc7d+7U97//feuUQT/4wQ+UlZWl+rB580653Qs1bdokrVv3pKKjeys5ebYKCk7RtaHbvp1Lbx/I0/T0zIA6dOnSpdtcnyfp0qVr0zA5YcIE/eMf/7D+vmHDBn3961/X6dOnNXz4cH366afWOSY3bdokuy1evF4TJ47VhAmjFRnZQxkZU9WmjUtr1myja0N3645sZTyxShu37A6oQ5cuXbrN9XmSLl26Ng2T+/fvV//+/a2/u91u/frXv7aGykcffdR6z+7f/e53mj17tuxUVnZO+/cfVkJC/BcLHhyshISB2rfvIN0AuwDQ0Jz2PEmXLl0bh8lWrVrpk08+sf5+5MgR3XjjjV63m48PHrR3sCksLFZ5eYXCw8O8rg8P76STJwvpBtgFgIbmtOdJunTp2jhMms3YK1assP4+aNAg7dixw+t2877cX/nKV2pslJaWqri42OtSWlrm66IAAADAae/NbTZnm4Ntjh07pmuvvVYPPPCAdu3apZiYGGuN5F/+8hfNnz+/xobZPJ6RkeF13Zw59+rBB++76P3DwjooJCRYBQXe07PZaTQiwnvK9gVdAGgcTnuepEuXro1rJs3Q+M9//lNlZWV6/PHHdebMGS1fvlwPPvigDh8+rJUrV+rOO++ssZGWlqaioiKvS1ra3dXePzS0tfr3j1RWVk7ldRUVFcrKytagQVf5+iXQBYBG5rTnSbp06dq4ZtLo27evtanb4/HoxIkT1gJFRESodevWdfp8l8tlXbyF1vg5U6aMV2rqXMXGRiouLkpLlmxQSclZJSaO9udLoHuRU3/07dWl8uNe3Tsrrl9PFZ46raPHCujSpUvX9q7Tnifp0qV7cUEeMxHa6OjRo5ozZ44yM309H1ntJ85ctmyTFi1aq/z8QsXE9FF6eori4wOfqFtCt22POTXePuLqGG1ddeFR+EtXv6aUGTXvtkCXLl26F1OS5707U1N/nqRLl+7FRKnBh8ns7GwNHjxY5eXlPn6mPWdhh3/DJADYrS7DJICmLsr+zdwbN26s8fbc3FxfkwAAAHAon4fJ8ePHKygoyNpfsjrmdgAAADR/Ph/N3bVrV+udbsxBNxe77N27t36WFAAAAM4fJocMGaI9e/ZUe3ttay0BAADQgjdzz5o1yzq3ZHUiIyOtd8EBAABA8+fzMGne/aYm7du3t95yEQAAAM2fz5u5AQAAgPMYJgEAAOA3hkkAAAD4jWESAAAAfmOYBAAAQMsYJpcvf1GjRiVrwIBEJSXNUE6OPe/nTVcaPixaz2fOVO6ueSrJW6FxY4basqx06dKl21yeJ+nSpWvTMPnb3/5W77//vhra5s075XYv1LRpk7Ru3ZOKju6t5OTZKig4RdeGbvt2Lr19IE/T0zMD6tClS5duc32epEuXrk3DpDlped++ffXNb35Tf/nLX1RWVqaGsHjxek2cOFYTJoxWZGQPZWRMVZs2Lq1Zs42uDd2tO7KV8cQqbdyyO6AOXbp06TbX50m6dOnauJl74cKF1snJb7/9dnXr1k3Tp0/Xv/71L9WXsrJz2r//sBIS4iuvCw4OVkLCQO3bd5BugF0AaGhOe56kS5euzcPkTTfdpPXr1+uDDz7Q//7v/2rLli2Kj4/XsGHD9Kc//UmffPKJ7FRYWKzy8gqFh4d5XR8e3kknTxbSDbALAA3Nac+TdOnSracDcC6//HJrmHznnXe0Y8cO9evXT/fff7+6du1a4+eVlpaquLjY61Ja2jCbywEAAGAfn4fJoKCgat+z+9lnn9WxY8c0d+7cGhtut1sdO3b0urjdC6q9f1hYB4WEBKugwHt6NjuNRkR4T9m+oAsAjcNpz5N06dK1cZj0eDw13t6hQwfdddddNd4nLS1NRUVFXpe0tLurvX9oaGv17x+prKycyusqKiqUlZWtQYOu8vVLoAsAjcxpz5N06dKtXiv5yPzjgXK5XNbFW2iNnzNlynilps5VbGyk4uKitGTJBpWUnFVi4uiAloXuF6f+6NurS+XHvbp3Vly/nio8dVpHjxXQpUuXru1dpz1P0qVL9+KCPLWtavTR0aNHNWfOHGVm+no+stpPnLls2SYtWrRW+fmFionpo/T0FMXHBz5Rt4Ru2x5zarx9xNUx2rpq9gXXL139mlJmzPd7WenSpdtyuyV5GY56nqRLl+7FRKnBh8ns7GwNHjxY5eXlPn6mPWdhh3/DJADYrS7DJICmLsr+zdwbN26s8fbc3FxfkwAAAHAon4fJ8ePHW0d017RCs7ojvgEAANC8+Hw0tzmH5Nq1a60DcS522bt3b/0sKQAAAJw/TA4ZMkR79uyp9vba1loCAACgBW/mnjVrls6cOVPt7ZGRkdq+fXugywUAAIDmOEyad7qpSfv27TVy5MhAlgkAAADNdZjE5zjVDgAAgB/7TAIAAADnMUwCAADAbwyTAAAA8BvDJAAAAFrGMLl8+YsaNSpZAwYkKilphnJyDjXZ7vBh0Xo+c6Zyd81TSd4KjRsz1JZlpUuXLt3m0nXa8zpdunRtHCY3bdqk2bNn6+9//7v18auvvqqbbrpJN9xwg/74xz+qPmzevFNu90JNmzZJ69Y9qejo3kpOnq2CglNNstu+nUtvH8jT9PTMgDp06dKl21y7Tntep0uXrk2nBlqwYIHuvfdexcfH6/e//73+8Ic/aOrUqfrud7+rkJAQTZ8+XSUlJfrpT38qOy1evF4TJ47VhAmjrY8zMqZqx45dWrNmm1JSkppcd+uObOtiN7p06dJtLl2nPa/TpUvXpjWTTz31lObNm6fdu3dr/fr1uuuuu/Too4/qT3/6k+bPn2/dZgZOO5WVndP+/YeVkBD/xYIHByshYaD27TvY5LoAgOb1vE6XLl0bh8kjR45o7Nix1t+/8Y1vqLy8XNddd13l7V//+tf1/vvvy06FhcUqL69QeHiY1/Xh4Z108mRhk+sCAJrX8zpdunRtHCbDw8Mrh8Vjx47ps88+U15eXuXt5rbLLrusxkZpaamKi4u9LqWlZb4uCgAAABqZz8PkLbfcouTkZD3yyCO69dZbNXnyZM2YMUMvvfSStmzZovvuu09jxoypseF2u9WxY0evi9td/abxsLAOCgkJVkGB9/RsdhqNiPCesn1RX10AQM2c9rxOly5dG4fJxx57zNqUvXLlSg0cONA6etsMl2bIvPHGG601l2ZYrElaWpqKioq8Lmlpd1d7/9DQ1urfP1JZWTmV11VUVCgrK1uDBl3l65dQ710AQM2c9rxOly5dG4/mbt++/QWn/5k5c6Z1hPe5c+d06aWX1tpwuVzWxVtojZ8zZcp4pabOVWxspOLiorRkyQaVlJxVYuLnRyX5q7665lQafXt1qfy4V/fOiuvXU4WnTuvosQK6dOnSbfFdpz2v06VL9+KCPB6PRzY6evSo5syZo8xMX89HVvuJM5ct26RFi9YqP79QMTF9lJ6eovj4wCdqf7pte8yp8fYRV8do66rZF1y/dPVrSpkx3+9lpUuXLl2ndEvyMhz1vE6XLt2LiVKDD5PZ2dkaPHiwdZS3b+w5C3tDqW2YBICWri7DJICmLsr+zdwbN26s8fbc3FxfkwAAAHAon4fJ8ePHKygoSDWt0DS3AwAAoPnz+Wjurl27au3atdZRQBe77N27t36WFAAAAM4fJocMGaI9e/ZUe3ttay0BAADQgjdzz5o1S2fOnKn29sjISG3fvj3Q5QIAAEBzHCZHjBhR63koR44cGcgyAQAAoLkOkwAA1AWnUHMmTumEet9nEgAAADiPYRIAAAB+Y5gEAACA3xgmAQAA0DKGyeXLX9SoUckaMCBRSUkzlJNzqMl2hw+L1vOZM5W7a55K8lZo3JihtiwrXbp06dKl25hdp70e063/rl/DZElJiTIzM/XDH/5QN954o771rW/pvvvu0yuvvKL6snnzTrndCzVt2iStW/ekoqN7Kzl5tgoKTjXJbvt2Lr19IE/T0zMD6tClS5cuXbpNqeu012O69dv169RAhw8f1ujRo62B0uVy6YMPPtBNN92kXbt26ZlnnlFiYqL+/Oc/q1Ure886tHjxek2cOFYTJoy2Ps7ImKodO3ZpzZptSklJanLdrTuyrYvd6NKlS5cu3cbsOu31mG79dv1aM/mTn/xEN9xwgz766CPl5eXJ7XZb78n9j3/8Q++88441VP7qV7+SncrKzmn//sNKSIj/YsGDg5WQMFD79h1scl0AAJojp70e022YecfnYfK1117TjBkzrPfgNu6//369/PLLKigo0Fe/+lU9+eSTWrJkiexUWFis8vIKhYeHeV0fHt5JJ08WNrkuAADNkdNej+k2zLzj87boTp066ZNPPqn8+NNPP9Vnn32m0NBQ6+O4uDh9+OGHNTZKS0utS1UuV5lcrs8bAAAAcAaf10x+85vf1M9+9jO9++67OnLkiO655x4NHDhQl156qXW72fR9+eWX19gwm8Y7duzodXG7F1R7/7CwDgoJCVZBgff0bHYajYjwnrJ9UV9dAACaI6e9HtNtmHnH52Hy8ccft9Yq9uvXT5GRkda+kosWLaq8PT8/X7NmzaqxkZaWpqKiIq9LWtrd1d4/NLS1+vePVFZWTuV1Zj/NrKxsDRp0la9fQr13AQBojpz2eky3YeYdnzdzm7WOWVlZeu+996yhMjo62uvI7e985zu1NsxR4ObireZN3FOmjFdq6lzFxkYqLi5KS5ZsUEnJWSUmfn5Ukr/qq2tOydC3V5fKj3t176y4fj1VeOq0jh4roEuXLl26dB3ZddrrMd367RpBHo/HIxsdPXpUc+bMsc5D6ZvaT5y5bNkmLVq0Vvn5hYqJ6aP09BTFxwc+UfvTbdtjTo23j7g6RltXzb7g+qWrX1PKjPl+LytdunTp0qVbn92SvAxHvR7Tre9uVMMPk9nZ2Ro8eLDKy8t9/Ex7zsLeUGobJgEAcKK6DJNoSaLs38y9cePGGm/Pzc31NQkAAACH8nmYHD9+vHWOyZpWaJ4/ByUAAACaN5+P5u7atavWrl1rHQV0scvevXvrZ0kBAADg/GFyyJAh2rNnT7W317bWEgAAAC14M7c5h+SZM2eqvd2ce3L79u2BLhcAAACa4zA5YsSIGm9v3769Ro4cGcgyAQAAoLlu5gYAAADOY5gEAACA3xgmAQAA4DeGSQAAAPiNYRIAAAANO0y++eab+v3vf6+0tDTrYv5urqtvy5e/qFGjkjVgQKKSkmYoJ+dQk+0OHxat5zNnKnfXPJXkrdC4MUNtWVa6dOnSpUu3MbtOez2mW/9dn4bJEydOWKcGuvrqqzV37ly9+uqr1sX83VxnbjP3qQ+bN++U271Q06ZN0rp1Tyo6ureSk2eroOBUk+y2b+fS2wfyND09M6AOXbp06dKl25S6Tns9plu/XZ/PMzl16lSVl5frnXfe0VVXXeV128GDB/XDH/5Q06ZN0+rVq2W3xYvXa+LEsZowYbT1cUbGVO3YsUtr1mxTSkpSk+tu3ZFtXexGly5dunTpNmbXaa/HdOu36/OayS1btugPf/jDBYOkYa576qmn9NJLL8luZWXntH//YSUkxFdeFxwcrISEgdq372CT6wIA0Bw57fWYbsPMOz4Nky6XS8XFxdXe/sknn1j3sVthYbHKyysUHh7mdX14eCedPFnY5LoAADRHTns9ptsw845Pw+R3v/td3XHHHVq3bp3XUGn+bq6bMmWKJk2aVGuntLTU+pyql9LSMv++AgAAADQan4bJ3/3ud7rxxhv1ve99T2FhYWrbtq11MX8315nbnnjiiVo7brdbHTt29Lq43QuqvX9YWAeFhASroMB7ejY7jUZEeE/ZvqivLgAAzZHTXo/pNsy84/Nm7meeeUb5+fl6+eWXlZmZaV3M38118+bNq9NmbnM6oaKiIq9LWtrd1d4/NLS1+vePVFZWTuV1FRUVysrK1qBBF+6/WVf11QUAoDly2usx3YaZd3w6mvu8Dh066Bvf+Ibf/6gZOC8cOkNr/JwpU8YrNXWuYmMjFRcXpSVLNqik5KwSEz8/Kslf9dU1p2To26tL5ce9undWXL+eKjx1WkePFdClS5cuXbqO7Drt9Zhu/XaNII/H4/HlE0pKSrRnzx5ddtll6tevn9dtZ8+e1apVqzR58mQ/FqX2E2cuW7ZJixatVX5+oWJi+ig9PUXx8YFP1P502/aYU+PtI66O0dZVsy+4funq15QyY77fy0qXLl26dOnWZ7ckL8NRr8d067sbZe8weejQIY0ZM0Z5eXkKCgrStddeqxUrVqhbt27W7cePH7f+bs5F6Tt7zsLeUGobJgEAcKK6DJNoSaLs3WcyNTVVsbGx1rvcmJOUX3rppdZAaYZLAAAAtDw+DZNvvPGGdSR2RESEIiMj9cILL2js2LHW2yjm5ubW31ICAADA+cOk2V+yVasvjtkxm7rN0d3jxo3TyJEjrc3gAAAAaDl8Opo7Ojpau3fvVkxMjNf1Tz/9tPXnzTffbO/SAQAAoPmsmbz11lutA24uxgyU5t1vfDw4HAAAAA7m86mB6o+zNpFzNDcAoDniaG7U66mB6pezhsn6wpAKADVj2AEcfGogAAAAoCqGSQAAAPiNYRIAAAB+Y5gEAABA0xgmCwsL9dxzz6m+LF/+okaNStaAAYlKSpqhnJxDLa47fFi0ns+cqdxd81SSt0Ljxgy1ZVnp0qVLt7l0nfa8Tpeu07u2DpPmPbqnTJmi+rB580653Qs1bdokrVv3pKKjeys5ebYKCk61qG77di69fSBP09MzA+rQpUuXbnPtOu15nS5dJ3d9fgec4uLiGm//5JNPVF8WL16viRPHasKE0dbHGRlTtWPHLq1Zs00pKUktprt1R7Z1sRtdunTpNpeu057X6dJ1ctfnNZOdOnVSWFhYtZfrrrtO9aGs7Jz27z+shIT4yuuCg4OVkDBQ+/YdbDFdAEDzel6nS9fJXb/WTF566aV64IEH9LWvfe2it7/33nu6++67ZbfCwmKVl1coPDzM6/rw8E7Kzf2gxXQBAM3reZ0uXSd3/RomBw8ebP05cuTIatdc1uUNdUpLS61LVS5XmVyuUF8WBwAAAI3Mp83ct912m9q0aVPt7V26dNGcObW/HaDb7VbHjh29Lm73gmrvHxbWQSEhwSooKPS63uw0GhHhPWX7wmldAEDzel6nS9fJXb+Gybvuuks/+clPqr39iiuuqNMwmZaWpqKiIq9LWlr1m8dDQ1urf/9IZWXlVF5XUVGhrKxsDRp0lS9fgqO7AIDm9bxOl66Tu35t5raLy+WyLt5q3sQ9Zcp4pabOVWxspOLiorRkyQaVlJxVYuLnRyX5y2ldcyqNvr26VH7cq3tnxfXrqcJTp3X0WAFdunTptviu057X6dJ1ctcI8tRlJ8cqSkpKtGfPHl122WXq16+f121nz57VqlWrNHnyZD8WpfYTZy5btkmLFq1Vfn6hYmL6KD09RfHxgU/UTanbtkfNa3ZHXB2jratmX3D90tWvKWXGfL+XlS5dunSd0i3Jy3DU8zpdus7uRtk7TB46dEhjxoyxTk4eFBSka6+9VitXrlTXrl2t248fP65u3bqpvLy8rsmqdT8+p/mpbZgEgJauLsMkALtE2bvPZGpqqmJjY3XixAkdPHjQOlXQ8OHDreESAAAALY9Pw+Qbb7xhHYkdERGhyMhIvfDCCxo7dqxGjBih3Nzc+ltKAAAAOH+YNPtLtmr1xTE7ZlP3M888o3HjxlnnnjSbwQEAANBy+HQ0d3R0tHbv3q2YmBiv659++mnrz5tvvtnepQMAAEDzWTN56623asWKFRe9zQyUkyZNqtM74AAAAKB58PnUQPWHTeQGR3MDQM04mhtoWkdzN8pJy9F81NeTOkM1cCGGKACO38wNAAAAVMUwCQAAAL8xTAIAAMBvDJMAAABo2GGyoqKi2uvr860Vly9/UaNGJWvAgEQlJc1QTs6hFtcdPixaz2fOVO6ueSrJW6FxY4basqz11TX4PtCl2zBdpz2f0aVLt3l0fRomi4uLNXHiRLVv315XXHGFZs+erfLy8srb8/Pz1bt3b9WHzZt3yu1eqGnTJmnduicVHd1bycmzVVBwqkV127dz6e0DeZqenhlQp6G6fB/o0m24rtOez+jSpev8rs/D5C9/+UtlZ2dr6dKleuSRR/Tcc8/plltuUVlZWeV96uu0lYsXr9fEiWM1YcJoRUb2UEbGVLVp49KaNdtaVHfrjmxlPLFKG7fsDqjTUF2+D3TpNlzXac9ndOnSdX7X52Fy/fr1WrBggb7zne/oRz/6kfXWimZtpHlv7tLS0sr367ZbWdk57d9/WAkJ8V8seHCwEhIGat++gy2m6zR8H4CG47TnM7p06Tq/W9ny5c5mcOzZs2flxxEREXr55Zf1ySef6KabbtKnn36q+lBYWKzy8gqFh4d5XR8e3kknTxa2mK7T8H0AGo7Tns/o0qXr/K5fw2SPHj30zjvveF136aWXauvWrSopKbHeu7suzFpMs/9l1Utp6RebygEAAOAMPg2TY8aM0eLFiy+4/pJLLtGWLVvUpk2bOnXcbrc6duzodXG7F1R7/7CwDgoJCVZBgff0bHYajYjwnrJ94bSu0/B9ABqO057P6NKl6/yuX8NkRkaGHnzwwYveZtZQbtu2Ta+++mqtnbS0NBUVFXld0tLurvb+oaGt1b9/pLKycrxOQ5SVla1Bg67y5UtwdNdp+D4ADcdpz2d06dJ1fve8Vr7cOSwszLpUxwyUI0eOrLXjcrmsi7fQGj9nypTxSk2dq9jYSMXFRWnJkg0qKTmrxMTRdV7+5tA1pxTp26tL5ce9undWXL+eKjx1WkePFTS5Lt8HunQbruu05zO6dOk6v2sEeXw8l4/ZN3LPnj267LLL1K9fP6/bzp49q1WrVmny5Ml+LErtJ85ctmyTFi1aq/z8QsXE9FF6eori4wOfqJtSt22POTXePuLqGG1dNfuC65eufk0pM+b7vaz+dkvyMmptt4TvA126Tn681QVdunRbajfK3mHy0KFD1n6T5l1uzCmArr32Wq1cuVJdu3a1bj9+/Li6devmdSLzurPnLOxOV9sQ1dTU5cWtJXwfACc/3gAgkGHSp30mU1NTFRsbqxMnTujgwYPWZu3hw4fX61soAgAAoOnyaZh84403rCOxzfklIyMj9cILL2js2LEaMWKEcnNz628pAQAA4Pxh0uwv2arVF8fsmE3dzzzzjPUOOObAG7MZHAAAAC2HT0dzR0dHW2+hGBMT43X9008/bf15880327t0AAAAaD5rJs073KxYseKit5mBctKkSfLx4HAAAAA4mM+nBqo/bCJ34lHMHM0NNByO5gbQ8Gw+NVD9YphE/WNIhZMxTAJw/KmBAAAAgKoYJgEAAOA3hkkAAAD4jWESAAAADTdMmuN1jhw5os8++8z6uKysTH/5y1/03HPP6eTJk6pPy5e/qFGjkjVgQKKSkmYoJ8eeg3bo0j1v+LBoPZ85U7m75qkkb4XGjRlqy7LSpdsQXac93ujSpds8uj4Nk+b9uHv37m29laI5cbkZKhMSEpScnKwf//jH1nXvvfee6sPmzTvldi/UtGmTtG7dk4qO7q3k5NkqKDhFl65t3fbtXHr7QJ6mp2cG1KFLtzG6Tnu80aVL1/ldn4fJ1NRUxcfH66233tK3v/1tfetb39KVV16pwsJCffzxx7rmmmv00EMPqT4sXrxeEyeO1YQJoxUZ2UMZGVPVpo1La9Zso0vXtu7WHdnKeGKVNm7ZHVCHLt3G6Drt8UaXLl3nd30eJt944w1lZGRowIAB+tWvfqV3331XM2fOVOvWreVyufTzn/9cr7/+uuxWVnZO+/cfVkJC/BcLHhyshISB2rfvIF26tnQBJ3Pa440uXbrO71a2fLnz6dOnddlll1l/b9++vXXp2rVr5e3du3fX8ePHZbfCwmKVl1coPDzM6/rw8E46ebKQLl1buoCTOe3xRpcuXed3z2vly527deumvLw89ejRw/r48ccf1+WXX155e35+vsLCvBf0YkpLS61LVS5XmVyuUF8WBwAAAI3MpzWTo0ePtjZtn2cOurn00ksrP966dasGDx5ca8ftdqtjx45eF7d7QbX3DwvroJCQYBUUeE/PZqfRiIjah1e6dIHmzmmPN7p06Tq/69cwOX/+fP3oRz+q9vbvfve7WrhwYa2dtLQ0FRUVeV3S0u6u9v6hoa3Vv3+ksrJyKq+rqKhQVla2Bg26ypcvgS5doFly2uONLl26zu/6tZm7Nua0QXVhDtYxF281b+KeMmW8UlPnKjY2UnFxUVqyZINKSs4qMXF0AEtMl+6Fp2zp26tL5ce9undWXL+eKjx1WkePFdCl26S7Tnu80aVL1/ldI8hjzkLug5KSEu3Zs8c6EKdfv35et509e1arVq3S5MmT/ViU2k+cuWzZJi1atFb5+YWKiemj9PQUxccHPlHTbTndtj3m1Hj7iKtjtHXV7AuuX7r6NaXMmO/3stKla0e3JC/DUY83unTpNodulL3D5KFDhzRmzBjrIJygoCBde+21WrlyZeUR3eZIbnOQTnl5eV2TVet+fA7gm9qGSaApq8swCQD2irL/pOWxsbE6ceKE9W445uCb4cOHW8MlAAAAWh6fT1pujsSOiIiw3lLxhRde0NixYzVixAjl5ubW31ICAADA+cOk2V+yVasvjtkxm7qfeeYZjRs3TiNHjrQ2gwMAAKDl8Olo7ujoaO3evVsxMTFe1z/99NPWnzfffLO9SwcAAIDms2by1ltv1YoVKy56mxkoJ02aJB8PDgcAAICD+XxqoPrDJnLUP47mhpNxNDcAxx/NDQAAAFTFMAkAAAC/MUwCAADAbwyTAAAA8BvDJAAAABp3mBw1apTef/991bfly1/UqFHJGjAgUUlJM5STY88R4HTpnjd8WLSez5yp3F3zVJK3QuPGDLVlWenSbYiu0x5vdOnSbR5dn4bJjRs3XvTy+uuva9OmTZUf14fNm3fK7V6oadMmad26JxUd3VvJybNVUHCKLl3buu3bufT2gTxNT88MqEOXbmN0nfZ4o0uXrvO7Pp9nMjg42HoLxZo+xdxeXl7ux6LUPB2bCXrAgK9q9ux7rI8rKio0cuQU3X77t5WSkuTHv0e3JXZ9Oc+kWWM08Ue/1Qtbd/u9nHTp2tmt7TyTTe3xRpcu3ebQtfk8k2PHjtWNN96ojz76yFqI85eQkBD961//sv7u3yBZs7Kyc9q//7ASEuK/WPDgYCUkDNS+fQfp0rWlCziZ0x5vdOnSdX63suXLnf/617/q+uuv19ChQ63N2g2lsLBY5eUVCg8P87o+PLyTTp4spEvXli7gZE57vNGlS9f53fNa+foJ999/v77xjW/o+9//vl544QXNnTvX53+0tLTUulTlcpXJ5Qr1uQUAAACHHc09cOBA7d6929o/0vzd17f3drvd6tixo9fF7V5Q7f3DwjooJCRYBQXe07PZaTQiwnvK9gVdukBz4bTHG126dJ3fDfjUQG3bttX8+fP1xBNP6L777lNERESdPzctLU1FRUVel7S0u6u9f2hoa/XvH6msrJzK68z+mVlZ2Ro06Cp/vwS6dIFmw2mPN7p06Tq/6/dm7i+7+eabrYsvXC6XdfFW8ybuKVPGKzV1rmJjIxUXF6UlSzaopOSsEhNH+7HUdOlWf8qWvr26VH7cq3tnxfXrqcJTp3X0WAFduk2667THG126dJ3f9fnUQEZJSYn27Nmjyy67TP369fO67ezZs1q1apUmT57sx6LUfuLMZcs2adGitcrPL1RMTB+lp6coPj7wiZpuy+nWdmqgEVfHaOuq2Rdcv3T1a0qZMd/vZaVL145ubacGamqPN7p06TaHbpS9w+ShQ4c0ZswY5eXlWftLXnvttVq5cqW6du1q3X78+HF169atXs4zCdjBl/NMAk1NXYZJALCXzeeZTE1NVWxsrE6cOKGDBw/q0ksv1fDhw63hEgAAAC2PT8PkG2+8YR2JbQ62iYyMtE4NZE5kPmLECOXm5tbfUgIAAMD5w6TZX7JVqy+O2TGbup955hmNGzdOI0eOtDaDAwAAoOXw6Wju6Oho6/ySMTExXtc//fTT1p++HtUNAACAFrRm8tZbb9WKFSsuepsZKCdNmuTzCcwBAADgXD6fGqj+sIkc9Y+jueFkHM0NoCkezR3wScsBJ6mvF2OGVABAS+X32ykCAAAADJMAAADwG8MkAAAA/MYwCQAAgMYbJo8cOaJt27bpX//6l+rb8uUvatSoZA0YkKikpBnKybHnCHC6dOu7O3xYtJ7PnKncXfNUkrdC48YMtWVZ6dJ18uOCLl26zaPr0zA5depUnT59uvLdcL7zne9Yb6to3lIxPj5eo0aNqrzdbps375TbvVDTpk3SunVPKjq6t5KTZ6ug4BRduk2+276dS28fyNP09MyAOnTpNqfHBV26dJ3f9XmYXLBggT799FPr7w8//LD++c9/6uWXX7YGyNdff115eXl65JFHVB8WL16viRPHasKE0YqM7KGMjKlq08alNWu20aXb5Ltbd2Qr44lV2rhld0AdunSb0+OCLl26zu/6PExWPb/5Cy+8oMcff1zf+MY31K5dOw0fPly/+93vtHbtWtmtrOyc9u8/rISE+MrrgoODlZAwUPv2HaRLt0l3gYbgtMcFXbp0nd+tbPn6CUFBQdafH330keLi4rxuM5u6jx49KrsVFharvLxC4eFhXteHh3fSyZOFdOk26S7QEJz2uKBLl67zu36/A84vf/lLa02kmWiPHTum/v37V95WUFCg9u3b19ooLS21LlW5XGVyuUJ9XRwAAAA0Ip/WTF533XU6ePCg9u3bp379+un999/3un3z5s1ew2V13G63Onbs6HVxuxdUe/+wsA4KCQlWQYH39Gx2Go2I8J6yfUGXbkN0gYbgtMcFXbp0nd/1a5jcsWOHtm/fXnn50Y9+5HX7bbfdpmXLltXaSUtLU1FRkdclLe3uau8fGtpa/ftHKisrp/K6iooKZWVla9Cgq3z5EujSbfAu0BCc9rigS5eu87t+b+auSZ8+fep0P5fLZV281byJe8qU8UpNnavY2EjFxUVpyZINKik5q8TE0QEsMV26DdM1p4Lp26tL5ce9undWXL+eKjx1WkePFdCla0vXaY8LunTpOr9rBHmqHqJdB+b8knv27NFll11mbequ6uzZs1q1apUmT57sx6LUfuLMZcs2adGitcrPL1RMTB+lp6coPj7wiZou3UC7bXvMqfH2EVfHaOuq2Rdcv3T1a0qZMd/vZaXbsroleRmOelzQpUu3OXSj7B0mDx06pDFjxljnkzRHdV977bVauXKlunbtat1+/PhxdevWTeXl5XVNVq378TlA01DbMAnYoS7DJADYK8refSZTU1MVGxurEydOWAfiXHrppdb5Jc1wCQAAgJbHp2HyjTfesI7EjoiIsN5G0Zy43LyV4ogRI5Sbm1t/SwkAAADnD5Nmf8lWrb44Zsds6n7mmWc0btw4jRw50toMDgAAgJbDp6O5o6OjtXv3bsXExHhd//TTT1t/3nzzzfYuHQAAAJrPmslbb71VK1asuOhtZqCcNGmS1/t3AwAAoHnz+dRA9YdN5HAujuZGQ+BobgANz+ZTA9UvhkngyxhS4WQMv0BzYPOpgQAAAICqGCYBAADgN4ZJAAAA+I1hEgAAAA0zTJaWlurcuXOVH//73//WAw88oNtvv13p6ek6cuSI6tPy5S9q1KhkDRiQqKSkGcrJseegHbp0ndodPixaz2fOVO6ueSrJW6FxY4basqx06TZE12mPN7p06dowTJq3TtywYYP197///e/q37+/Nm3aZA2Ymzdvtt63OysrS/Vh8+adcrsXatq0SVq37klFR/dWcvJsFRScoku3xXbbt3Pp7QN5mp6eGVCHLt3G6Drt8UaXLl0bhsl9+/YpPj7e+rtZIzl16lRlZ2dr5cqV2rt3r372s59p1qxZqg+LF6/XxIljNWHCaEVG9lBGxlS1aePSmjXb6NJtsd2tO7KV8cQqbdyyO6AOXbqN0XXa440uXbo2DJPl5eXWxXj33Xd1xx13eN1+5513WsOl3crKzmn//sNKSPh8kDWCg4OVkDBQ+/YdpEu3RXYBJ3Pa440uXbo2DZNf+9rX9MILL1h/79u37wWD41tvvaXLLrtMdissLFZ5eYXCw8O8rg8P76STJwvp0m2RXcDJnPZ4o0uXbvVayQe/+tWvdOONN+rMmTPW+3DPmDFD7733nmJiYnTw4EE99dRTSktLq9OBPOZSlctVJpcr1JfFAQAAQCPzaZi85ppr9Ne//tXaN/Kf//yndd0jjzxi/dmtWzc9+OCD+ulPf1prx+12KyPD+2225sy5Vw8+eN9F7x8W1kEhIcEqKPCens1OoxER3lO2L+jSdXIXcDKnPd7o0qVr43kmzUBpjtg+fvy49ac5qjs3N1cffPBBnQZJw6y9LCoq8rqkpd1d7f1DQ1urf/9IZWXlVF5XUVGhrKxsDRp0la9fAl26zaILOJnTHm906dK1ac1kVZ07d7Yu/nC5XNbFW82buKdMGa/U1LmKjY1UXFyUlizZoJKSs0pMHO3XMtCl2xy65pQtfXt1qfy4V/fOiuvXU4WnTuvosQK6dJt012mPN7p06V5ckMfj8cgHJSUl2rNnj3WgTb9+/bxuO3v2rFatWqXJkyfLd7WfOHPZsk1atGit8vMLFRPTR+npKYqPD3yipku3qXbb9phT4+0jro7R1lWzL7h+6erXlDJjvt/LSpeuHd2SPO/dmZr6440uXboXEyVbh8lDhw5pzJgxysvLU1BQkK699lrrHJNdu3a1bjebvs2+k+dPH+Qbe87CDjQntQ2TQFNWl2ESQFMXZe8+k6mpqda73Jw4ccI6evvSSy/V8OHDreESAAAALY9Pw+Qbb7xhHYkdERGhyMhI65yT5i0WR4wYYR2EAwAAgJYl2Nf9JVu1+uKYHbOp+5lnntG4ceM0cuRIazM4AAAAWg6fjuaOjo7W7t27rZOUV/X0009bf9588832Lh0AAACaz5rJW2+9VStWrLjobWagNO+K4+PB4QAAAHAwn08NVH/YRA58GUdzw8k4mhtoDmw+NVD9YpgEGgpDKqpi6APQYKcGAgAAAKpimAQAAIDfGCYBAADgN4ZJAAAANMwwuWbNGn366adqLMuXv6hRo5I1YECikpJmKCfHnoN26NKl6234sGg9nzlTubvmqSRvhcaNGWrLstJ1ZtdpP7906dJt2K5Pw2RSUpK6du2qlJQU/fOf/1RD2rx5p9zuhZo2bZLWrXtS0dG9lZw8WwUFp+jSpWtzt307l94+kKfp6ZkBdeg2j67Tfn7p0qXbcF2/NnPPnDnTeheca665RrGxsXryySdVUFCg+rZ48XpNnDhWEyaMVmRkD2VkTFWbNi6tWbONLl26Nne37shWxhOrtHHL7oA6dJtH12k/v3Tp0m24rl/D5N133629e/dq165duu6665SRkaGvfOUrmjhxorZtC3yBLqas7Jz27z+shIT4yuuCg4OVkDBQ+/YdpEuXro1dwMk/v3Tp0m341wu/D8AZMmSI5s2bpw8//FB/+tOflJ+frxtuuEG9e/eW3QoLi1VeXqHw8DCv68PDO+nkyUK6dOna2AWc/PNLly7dhn+9aOXLnYOCgi64rk2bNrr99tuty+HDh7V48eJaO6WlpdalKperTC5XqC+LAwAAgEbm05rJ2t55MTIyUo888kitHbfbrY4dO3pd3O4F1d4/LKyDQkKCVVDgPT2bnUYjIrynbF/QpUsXaF4/v3Tp0m341wufhskjR46oc+fOAf+jaWlpKioq8rqkpd1d7f1DQ1urf/9IZWXlVF5XUVGhrKxsDRp0ld/LQZcuXaB5/fzSpUu34V8vfNrM3bNnT9nB5XJZF281b+KeMmW8UlPnKjY2UnFxUVqyZINKSs4qMXF0QMtCly7di59ipm+vLpUf9+reWXH9eqrw1GkdPeb/2RvoOrPrtJ9funTpNlzX52HSKCkp0Z49e3TZZZepX79+XredPXtWq1at0uTJk2W3m24aoY8/LtJTTy1Xfn6hYmL6aOHCjIBXz9KlS/dCg+P6aOuq2ZUfPz7n88f00tWvKWXGfLotrOu0n1+6dOk2XNcI8tS2I2QVhw4d0pgxY5SXl2cdjHPttddq5cqV1onMjePHj6tbt24qLy/3Y1HsOQs7gNq17TGnsRcBTUhJXkZjLwKAJivK3n0mU1NTrROVnzhxQgcPHtSll16q4cOHW8MlAAAAWh6fhsk33njDOhI7IiLCOnL7hRde0NixYzVixAjl5ubW31ICAADA+cOk2V+yVasvdrM0m7qfeeYZjRs3TiNHjrQ2gwMAAKDl8OkAnOjoaOt9uWNiYryuf/rpp60/b775ZnuXDgAAAM1nzeStt96qFStWXPQ2M1BOmjSp1hObAwAAoPnw6Wju+sUmcqChcDQ3quJobgCBHM1t1iQ6ytmzZz1z5syx/qRLly5dunTp0qXbuN0mtGayboqLi6338jZvwdihQwe6dOnSpUuXLl26jdj1aZ9JAAAAoCqGSQAAAPiNYRIAAAAtZ5h0uVyaM2eO9SddunTp0qVLly7dxu067gAcAAAANB2OWzMJAACApoNhEgAAAH5jmAQAAIDfGCYBAADQMobJP/zhD+rVq5fatGmjr33ta3rzzTcDbr7++usaN26cunXrpqCgIK1fvz7gptvt1v/8z//o0ksv1eWXX67x48fr4MGDAXefeeYZxcXFWWesN5drrrlGf/3rX2W3Rx991PpeTJ8+PaDOgw8+aHWqXqKjo21Zxv/+97/6wQ9+oPDwcLVt21YDBgzQ7t27A2qan60vL6+5TJs2LaBueXm5fvnLX6p3797Wsvbt21cPP/yweStTBeqTTz6x/p969uxptRMSErRr1y5bHwNmOWfPnq2uXbta/8bo0aP13nvvBdxdu3atxowZY/0fmtvfeuutgJf33LlzSk1NtX4e2rdvb91n8uTJOnbsWMDLa36ezc+v6YaFhVnfh3/+858Bd6u65557rPs8+eSTAXfvvPPOC36Wb7jhBluW95133tHNN99svYuG+X6Y57u8vLyAuhd77JnLb37zm4C6p0+f1r333qsrr7zS+vnt16+f5s+fH/D34fjx49b32Nzerl0763tbl8dFXV4fzp49az3vmMfGJZdcogkTJlj/XqDdP/7xj/r6179uvX6Yr+nUqVMBL+/HH3+s++67T1dddZX1/e3Ro4d+8pOfWO+uEujy3n333dbzpel27txZt9xyi959992Au1Wf22688cY6vfbXpfv1r3/9gp9f85i2Y3mzsrI0atQo6/Fm/v+uu+46lZSU+N39z3/+U+1jbvXq1Wq2w+Rf/vIX/exnP7MOZ9+7d6/i4+M1duxYnThxIqDumTNnrJYZVO3y2muvWU8E//jHP7Rt2zbrBc68aJp/KxDmydAMenv27LEGJ/ODZR5c+/fvt23ZzSCyYMECa2i1Q//+/fXhhx9WXv72t78F3CwsLNTw4cPVunVra5g+cOCAfvvb31ov8IF+7VWX1fzfGUlJSQF1H3vsMesXgaefftp6ETYfP/744/q///s/BepHP/qRtZxLly7V22+/bf2cmSHHDNt2PQbMsj711FPWC7AZnsyTmXnsmRe8QLrm9muvvdb6fviipu6nn35qPT+Y4d38aQZW8wRqBp9AukZUVJT1f2i+z+bn2PzyYb7f+fn5AXXPW7dunfWcYYaTuqhL1ww4VX+mV6xYEXD33//+t/X/ZgbrHTt2KCcnx/p+m1/yA+lWXU5zyczMtF7YzCAVSNe8brz00ktatmyZ9fgzv3yZ4XLjxo1+d80QYl6cc3NztWHDBu3bt8/6hc489mp7nq/L68P999+vF154wXpRN/c3vwwlJiYG3DWPD/Mz8Ytf/KLGli9ds2zm8sQTT+hf//qXnn32Wev7nZycHPDyDhkyRIsXL7b+37Zs2WJ93819zC/ogXTPM7+0mZ8xO74P5911111eP8fm+TPQrhkkzf+bud6sRDOvVeZnODg42O9u9+7dL3jMZWRkWL+8mAHbZx6HGDZsmGfatGmVH5eXl3u6devmcbvdtv0b5tuxbt06j91OnDhhtV977TXb22FhYZ6FCxfa0vrkk088X/3qVz3btm3zjBw50vPTn/40oJ55I/n4+HiP3VJTUz3XXnutp76Zr79v376eioqKgDrf+ta3PD/84Q+9rktMTPR8//vfD6j76aefekJCQjybNm3yun7w4MGeBx54wJbHgPnau3Tp4vnNb35Ted2pU6c8LpfLs2LFCr+7VR05csS6fd++fQEv78W8+eab1v3ef/99W7tFRUXW/V5++eWAux988IHnK1/5iudf//qXp2fPnp65c+fWuVld94477vDccsstPnXq0v3ud7/r+cEPfmB798vMso8aNSrgbv/+/T0PPfRQQI+RL3cPHjxoXWf+v6q+JnXu3Nnzpz/9KaDXB/P4at26tWf16tWV93nnnXes+2RlZfndrWr79u3WbYWFhT4ta23d81atWuUJDQ31nDt3ztZudna2dZ/Dhw8H3DXPN+Yx9+GHH/r12n+x7kgbXjcv1v3a177mSU9Pt737ZQMHDrzgtaquHLFmsqyszFobZ37rO89M5OZjM7E3dedX91922WW2Nc1vZitXrrR+yzCbu+1gfov51re+5fV9DpTZ7GPWtPTp00ff//73a90UVhdmjcLQoUOtNYZm9f2gQYP0pz/9SXb/zJk1GT/84Q/r/Jtrdcym51deeUWHDh2yPs7OzrbWbPn1218Vn332mfVz8OU1QmaTkB1rgI0jR47oo48+8vqZMJs2zW4mTnjsnX/8mf/DTp062frzYTYZmu+FWXsViIqKCt1+++2aNWuWtSbfTmbNoXmMmE2QP/7xj1VQUBDwsr744ovWWlqzdtq0zc+CHbsHVWU26Zp/p7a1W3V9/JnnDLO23syF27dvtx6LZi2Nv0pLS60/qz72zGuSOQm0r4+9L78+mNc6sxap6mPOrAU2m499eczVx+tOXbvmPmZTbKtWrWzrmtc6s5bS7C5k1qoF0jVraG+77TZrrXOXLl3q3KrL8i5fvlwRERGKjY1VWlqa9W8F0jVbX80WIfNYMz/LV1xxhUaOHBnwz9mXmZ87s6uR3485jwP897//tSbqN954w+v6WbNmWWssm/KaSfPbqlkzNXz4cFt6OTk5nvbt21trpDp27Oh58cUXbematUyxsbGekpIS237D2rx5s/Ubqvlt8qWXXvJcc801nh49eniKi4sD6pq1YuaSlpbm2bt3r2fBggWeNm3aeJ599lmPXf7yl79Y32Pzs2fHz4BZmxoUFORp1aqV9eevf/1rW5bTfE/N/5VZzs8++8yzdOlST3BwsCcqKsqWx8Df//5367pjx4553S8pKckzceJEv7sNtWbS/DybtVC33XabLd0XXnjBevyZ/0OzZcSs9Qy0a34WvvnNb1auAbdrzaR5TG/YsMF6zjC3xcTEeP7nf/7H+jnxt3t+LU67du08v/vd76z/M7N1yHw/duzYEdDyVvXYY49ZW13OPx8F0j179qxn8uTJ1m3m8WfWmC1ZsiSgbllZmfVcZh4HH3/8sae0tNTz6KOPWvcbM2ZMQK8Py5cvt5bxy8z/3f/+7//63bVjzWRdXs/y8/Ot780vfvELW7p/+MMfrMecWd6rrrrKp7WS1XVTUlI8ycnJfr/2V9ddsGCB9VpnHnPLli2z1nzeeuutAXXN2mizfJdddpknMzPTes2bPn269TNy6NChgJa3qh//+MfWc4S/GCbreZi85557rBeHo0eP2tIzT1rvvfeeZ/fu3Z6f//znnoiICM/+/fsDaubl5Xkuv/xya+g7z45h8svME1eHDh0C3ixvNgGZIaqq++67z3P11Vd77GJeEL797W/b0jIv6ldeeaX1p3mSee6556wnBjuGX/PEet1111k/u2b4NS84ZvN5dHS0p6UPk+YFf9y4cZ5BgwZZm6Tt6J4+fdp6/JkneLM5qFevXp7jx4/73TWP4yuuuMLrlxa7hskv+/e//x3wZvnzz8WTJk3yup/5Pn/ve9+zbXnN0HDvvffWuVdT1+yiYX652rhxo/Uc93//93+eSy65xNqdJ5Cu+b8zu/Gcf+yNHTvWc+ONN3puuOGGgF4f7Bgma3vd8XeYrK1rHmfmNdl8D8zjz46u2exvhiazedb8nJlfDuv6S8bFuuYXrMjISGu3Ln9f++v6uv7KK6/4tFn+Yt3zz8Fm5UlVAwYMsGYAO5bX7DJlVk498cQTnmY9TJoByjxYv/yfbX7bvPnmm5vsMGn28TRDRG5urqe+XH/99dZvWYEwX/P5J8TzF/OxWdtg/u7LmozaDB06tM4PgOqY33qr/lZpzJs3z1pTZIf//Oc/1tq99evX29IzPwNPP/2013UPP/yw9YJpFzPknB/4zJB300032fIYOD+AfHnQMwPsT37yE7+79T1Mmhey8ePHe+Li4jwnT560rftl5kXJl7XMX+6aofH846zqY8/8/Jknf7uX1/zyOX/+fL+75rnYrN0zP79VmSEnISHBluV9/fXXrdvfeuutOveq65oXSfPL55f3KzbPH2b4s2N5zbBj9kczzCA1derUgF4fzg8gXx70zPOeWRvsbzfQYbK2rtniZH7JN69JvqxR9uV10vz8mbXif/7zn/3umpUk1T3mzEoUO5f39OnTVtusrfS3az42DbPVqSrzPF+XLS51WV6zgsM8Ts7/HDfbfSZDQ0Oto7rMfmdV990xH9u1v6CdzHOPOdLKHJ356quvWvt41BfzfTi//46/rr/+eusIVbO/xPmL2SfR7ONo/h4SEmLLsppTdJgjQc0pZgJhjuT+8qkTzD5Q5mhKO5j9csz+KWb/UTuYfWa+fNSd+Z6a/zu7mCOszffVHOlujno0R/nbwfzsmn2Kqj72iouLrX14muJjzzD7m02cONHaX/fll1+2Tq/SVB9/Zl9JczR01cee2cfY7D9p/h/t9MEHH1j7TAby+DPPxeZ0I/X5+Fu0aJH1fB/ovqjnfxbMpT4ff2a/WXPaGvPzZs6yUdtjr7bXB/O1mzNVVH3Mme+32d+8psdcfb3u1KVrnhPMPqjm58Psn1rbkf3+Lu//XwFW42Outu7Pf/7zCx5zxty5c63nfjuX963/367pMVdb15w1wjwn+PqY82V5zWPOnPHC/Bz7zeMQK1eutPaTM5sGDxw4YK2N69Spk+ejjz4KqGtWdZs1IuZivh3n9wPy5cjPi+17YFYZm32IzD5G5y/mt+RAmDV6ZlW/WZNjNpeaj81vWFu3bvXYzY7N3DNmzLC+B2Z5zar60aNHW2tGAvntxzD7qZm1I4888oi1ydFsFjK/rZp9VAJl9i0xawDMPo52MUfVmn1nzNoR871Yu3at9X2o6yarmpjfeP/6179av3WanwOz2c0c+efLJqbaHgNmXzDzWDu//505yrZ37961rn2orVtQUGB9bPb7Nbebx7j52DxW/O2ar9tsrTC/iZs1W1Uff2athr9ds4bBbGYym7fNmmuziXPKlCnWc1LVI3r9+T58WV03c9fUNbfNnDnTWl7zM2c2bZvNg+ZsDWYfwkCW1/z8mrUYf/zjH63Hn9lsbNbu7Ny5M+Dvg9lMah7LzzzzTK1ff1275rnMHNFt1saZx8nixYutfazN1oxAumZ/cNM0a+/NVgzz/2bO0mDH64PZLGmeh1599VXrZ82s8fvyrj3+dM3H5mswR5ybr8msBTYfm8eiv13zf2aec8xmV7M5t+p9atqqVVvXfF/NWn/z9ZvvuXkNMZu5zS5CNe1a4s/rb13W7NfWPXz4sHXWALO85jFnni/79OljbcUJpGuY5wOzi5g5wt885syR3eZnuKbN53X9PpiemSPM60ggHDNMGuZJyzzAzP4kZnPCP/7xj4Cb51f3f/liBgB/XaxnLuZJLBBmHy3zhGW+fnMKCrM5oT4GSbuGSXMKka5du1rLa4Yp87EvO0/XxBwIYQ4YMi/mZv9A88Jmhy1btlj/V+bUH3Yxm3/M99L87JonAPMEY05LUttwU9cDhUzPfI/NKXzMJg2z2c3Ox4A5MOSXv/yltW+f+X6bn7u6fH9q65rHw8VuN6eU8rd7fpP5xS7m8/ztmsHZ7EhvdqUw32vzc22G1rocgOPrc0xdh8mauuYFw+z3a54nzOBnmnfddVedfvmuy/IuWrTI2sRvfp7NLzB12SWkLl1zAEPbtm19+hmurWteQO+8807r/84sr9m95Le//W2tp/yqrfv73//e+qXFfH/NY9u8wNflMV2X1wfz82Y2l5uDkMxwbX72avslqy5d89jy9bWptm513ydzMY9Hf7tm/1yzD6rZn998j8332mzWfffddwP+PvgzTNbWzcvLswZHM+ya50nz+DDHddS2v3Zdl9cc6Ga+B+bnwfxiUdsvb3Xtml+Su3fvbq1ICUTQ//9HAQAAAJ85Yp9JAAAANE0MkwAAAPAbwyQAAAD8xjAJAAAAvzFMAgAAwG8MkwAAAPAbwyQAAAD8xjAJAAAAvzFMAgAAwG8MkwAAAPAbwyQAAAD8xjAJAAAA+ev/AZWDoxckm2JHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(mask, annot=True, fmt=\"d\", cmap=\"YlGnBu\", cbar=False, square=True)\n",
    "plt.title(\"台北市有效網格 Mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ec5dc",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c559f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os.path\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "def evaluate_model(model, MSE_criterion, DataLoader, device, configs, is_save, itr=999):\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'test...')\n",
    "\n",
    "    res_path = os.path.join(configs.gen_frm_dir, str(itr))\n",
    "    if not os.path.exists(res_path):\n",
    "        os.mkdir(res_path)\n",
    "    avg_mse = 0\n",
    "    avg_rmse = 0\n",
    "    avg_mae = 0\n",
    "    correct_count = 0\n",
    "    item_count = 0\n",
    "    batch_id = 0\n",
    "\n",
    "    loss_list = []\n",
    "    model.eval()\n",
    "    all_tre = []\n",
    "    # only taipei\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, item in enumerate(DataLoader):\n",
    "            print(\"batch\", batch)\n",
    "            batch_id = batch_id + 1\n",
    "            item_count = item_count+1\n",
    "            xc, xp, xt, ys, yd = item\n",
    "            xc, xp, xt, ys, yd = list(map(lambda x: x.to(device), [xc, xp, xt, ys, yd]))\n",
    "            pred = model(xc, xp, xt, yd)\n",
    "            # 準備 mask (B, 1, H, W)\n",
    "            B, C, H, W = ys.shape\n",
    "            valid_mask = torch.tensor(mask, dtype=ys.dtype, device=device).unsqueeze(0).unsqueeze(0)\n",
    "            valid_mask = valid_mask.expand(B, C, H, W)\n",
    "            valid_mask_np = valid_mask.cpu().numpy()\n",
    "            \n",
    "            print(\"valid_mask shape:\", valid_mask.shape)\n",
    "            pred = pred * valid_mask\n",
    "            ys = ys * valid_mask\n",
    "            print(\"pred shape:\", pred.shape, \"ys shape:\", ys.shape)\n",
    "            # # masked diff\n",
    "            # diff = (pred - ys) * valid_mask\n",
    "            # # loss\n",
    "            # mse = torch.mean(diff ** 2)\n",
    "            # mae = torch.mean(torch.abs(diff))\n",
    "            # rmse = torch.sqrt(torch.mean(diff ** 2))\n",
    "            \n",
    "            loss = MSE_criterion(pred, ys)\n",
    "            loss_item = loss.cpu().item()\n",
    "            loss_list.append(loss_item)\n",
    "            #print(\"validation:batch \",batch,\"loss=\",loss_item)\n",
    "            # MSE per frame\n",
    "            x = ys.cpu().numpy() * configs.max_value  # (B, H, W, C)\n",
    "            gx = pred.cpu().numpy() * configs.max_value\n",
    "            # gx[gx < 1] = 0\n",
    "            \n",
    "            mae = np.mean(np.abs(x - gx))\n",
    "            mse = np.mean(np.square(x - gx))\n",
    "            rmse = np.mean(np.square(x - gx))\n",
    "            \n",
    "            avg_mae += mae\n",
    "            avg_mse += mse\n",
    "            avg_rmse += rmse\n",
    "            \n",
    "            # TRE\n",
    "            \n",
    "            B = x.shape[0]  # 每個 batch 中有 B 筆資料\n",
    "            for i in range(B):\n",
    "                valid = valid_mask_np[i]\n",
    "                correct = np.sum((np.abs(x[i] - gx[i]) < 1) * valid)\n",
    "                tre = (1 - correct / count) * 100\n",
    "                all_tre.append(tre)\n",
    "                # draw \n",
    "                if TRE_draw == True:\n",
    "                    error = x[i] - gx[i]\n",
    "                    error = np.squeeze(x[i] - gx[i])\n",
    "                    \n",
    "                    fig = sns.heatmap(error, annot=True,annot_kws={\"size\": 3}, fmt=\".3f\", vmin=0, xticklabels=False,\n",
    "                        yticklabels=False, cbar=False, cmap='rainbow', square=True)\n",
    "                    heatmap = fig.get_figure()\n",
    "                    heatmap.savefig(os.path.join(\"error_map\", f\"error_map_{i}.png\"), dpi=300, bbox_inches='tight')\n",
    "                    plt.clf()\n",
    "                    # debug\n",
    "                    fig = sns.heatmap(np.squeeze(x[i]), annot=True,annot_kws={\"size\": 3}, fmt=\".3f\", vmin=0, xticklabels=False,\n",
    "                        yticklabels=False, cbar=False, cmap='rainbow', square=True)\n",
    "                    heatmap = fig.get_figure()\n",
    "                    heatmap.savefig(os.path.join(\"error_map\", f\"true_{i}.png\"), dpi=300, bbox_inches='tight')\n",
    "                    plt.clf()\n",
    "                    # prediction\n",
    "                    fig = sns.heatmap(np.squeeze(gx[i]), annot=True,annot_kws={\"size\": 3}, fmt=\".3f\", vmin=0, xticklabels=False,\n",
    "                        yticklabels=False, cbar=False, cmap='rainbow', square=True)\n",
    "                    heatmap = fig.get_figure()\n",
    "                    heatmap.savefig(os.path.join(\"error_map\", f\"prediction_{i}.png\"), dpi=300, bbox_inches='tight')\n",
    "                    plt.clf()\n",
    "            # save prediction examples\n",
    "            if batch_id <= configs.num_save_samples:\n",
    "                path = os.path.join(res_path, str(batch_id))\n",
    "                if not os.path.exists(path):\n",
    "                    os.mkdir(path)\n",
    "                for i in range(configs.input_length):\n",
    "                    name = 'gt' + str(i + 1) + '.png'\n",
    "                    file_name = os.path.join(path, name)\n",
    "                    img_c = xc[0, i, 0, :, :].cpu().numpy()\n",
    "                    draw_pic(img_c, file_name, configs.max_value)\n",
    "\n",
    "                name = 'pd7' + '.png'\n",
    "                file_name = os.path.join(path, name)\n",
    "                img_pd = pred[0, 0, :, :].cpu().numpy()\n",
    "                draw_pic(img_pd, file_name, configs.max_value)\n",
    "\n",
    "                name = 'gt7' + '.png'\n",
    "                file_name = os.path.join(path, name)\n",
    "                img_gt = ys[0, 0, :, :].cpu().numpy()\n",
    "                draw_pic(img_gt, file_name, configs.max_value)\n",
    "\n",
    "        avg_mse = avg_mse / batch_id\n",
    "        avg_mae = avg_mae / batch_id\n",
    "        avg_rmse = np.sqrt(avg_rmse / batch_id)\n",
    "        \n",
    "        # TRE\n",
    "        # total_pixels = x.size * batch_id\n",
    "        # tre = (1 - correct_count / total_pixels) * 100\n",
    "        sum_tre = np.sum(all_tre)\n",
    "        print(\"sum_tre:\", sum_tre)\n",
    "        print(\"count:\", count)\n",
    "        avg_tre = sum_tre / count\n",
    "        print('mse per frame: {}, rmse per frame: {}, mae per frame: {}, avg_tre : {} %'.format(avg_mse, avg_rmse, avg_mae,avg_tre))\n",
    "        if(is_save):\n",
    "            save_dir = \"report\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            now_str = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            log_path = os.path.join(save_dir, now_str + \".txt\")\n",
    "            with open(log_path, \"w\") as log_file:\n",
    "                log_file.write('mse per frame: {}, rmse per frame: {}, mae per frame: {}, avg_tre : {} %'.format(avg_mse, avg_rmse, avg_mae,avg_tre))\n",
    "        # save tre\n",
    "        with open(\"report/individual_tre.txt\", \"w\") as f:\n",
    "            for i, tre in enumerate(all_tre):\n",
    "                f.write(f\"Sample {i}: TRE = {tre:.2f}%\\n\")\n",
    "    return avg_mse, avg_rmse, avg_mae, tre\n",
    "\n",
    "\n",
    "def draw_pic(img_gt, file_name,denorm_factor=1.0 ):\n",
    "    img_gt[img_gt > 0.2] = 0.2\n",
    "    factor = 1.0 / 3.0\n",
    "    img_gt[(img_gt > 0.05) & (img_gt <= 0.2)] = img_gt[(img_gt > 0.05) & (img_gt <= 0.2)] * factor + 2 * factor * 0.05\n",
    "    img_gt = img_gt * denorm_factor\n",
    "    fig = sns.heatmap(img_gt, annot=True,annot_kws={\"size\": 3}, fmt=\".3f\", vmin=0, xticklabels=False,\n",
    "                      yticklabels=False, cbar=False, cmap='rainbow', square=True)\n",
    "    heatmap = fig.get_figure()\n",
    "    heatmap.savefig(file_name, dpi=300, bbox_inches='tight')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd882ab",
   "metadata": {},
   "source": [
    "# evaluate_model_with_custom_YD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ca1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_custom_YD(model, DataLoader, MSE_criterion, device, target_feature_idx, configs):\n",
    "    print(\"evaluate_model_with_custom_YD:\", target_feature_idx)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_count = 0\n",
    "    batch_id = 0\n",
    "    avg_mse = 0\n",
    "    avg_rmse = 0\n",
    "    avg_mae = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, item in enumerate(DataLoader):\n",
    "            batch_id = batch_id + 1\n",
    "            xc, xp, xt, ys, yd = item\n",
    "            XC = xc.to(device)\n",
    "            XP = xp.to(device)\n",
    "            XT = xt.to(device)\n",
    "            YS = ys.to(device)\n",
    "            YD = yd.to(device)\n",
    "            print(\"YD shape:\", YD.shape)\n",
    "\n",
    "            YD_modified = YD.clone()\n",
    "            YD_modified[:, target_feature_idx] = 0  \n",
    "\n",
    "            pred = model(XC, XP, XT, YD_modified)\n",
    "            loss = MSE_criterion(pred, YS)\n",
    "            # MSE per frame\n",
    "            x = ys.cpu().numpy() * configs.max_value  # (B, H, W, C)\n",
    "            gx = pred.cpu().numpy() * configs.max_value\n",
    "            # gx[gx < 1] = 0\n",
    "            \n",
    "            mae = np.mean(np.abs(x - gx))\n",
    "            mse = np.mean(np.square(x - gx))\n",
    "            rmse = np.mean(np.square(x - gx))\n",
    "            \n",
    "            avg_mae += mae\n",
    "            avg_mse += mse\n",
    "            avg_rmse += rmse\n",
    "            # TRE\n",
    "            correct_count += np.sum(np.abs(x - gx) < 1)\n",
    "    avg_mse = avg_mse / batch_id\n",
    "    avg_mae = avg_mae / batch_id\n",
    "    avg_rmse = np.sqrt(avg_rmse / batch_id)\n",
    "\n",
    "    # TRE\n",
    "    total_pixels = x.size * batch_id\n",
    "    tre = (1 - correct_count / total_pixels) * 100\n",
    "    return avg_mse, avg_rmse, avg_mae, tre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87b2c0",
   "metadata": {},
   "source": [
    "# trainer_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ec1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, loss_func, dataloader, lr_scheduler, epoch, device):\n",
    "    loss_list = []\n",
    "    model.train()\n",
    "    print(\"The learning rate of the %dth epoch：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    for batch, item in tqdm(enumerate(dataloader)):\n",
    "        xc, xp, xt, ys, yd = item\n",
    "        xc, xp, xt, ys, yd = list(map(lambda x: Variable(x.to(device)), [xc, xp, xt, ys, yd]))\n",
    "        optimizer.zero_grad()\n",
    "        next_frame = model(xc, xp, xt, yd)\n",
    "        loss = loss_func(next_frame, ys)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_item = loss.cpu().item()\n",
    "        #print(\"train: batch \",batch,\"loss=\",loss_item)\n",
    "        loss_list.append(loss_item)\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(epoch)\n",
    "\n",
    "    return sum(loss_list) / len(loss_list)\n",
    "\n",
    "\n",
    "def test_epoch(model, loss_func, DataLoader, device):\n",
    "    loss_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, item in tqdm(enumerate(DataLoader)):\n",
    "            xc, xp, xt, ys, yd = item\n",
    "            xc, xp, xt, ys, yd = list(map(lambda x: x.to(device), [xc, xp, xt, ys, yd]))\n",
    "            next_frame = model(xc, xp, xt, yd)\n",
    "\n",
    "            loss = loss_func(next_frame, ys)\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "    return sum(loss_list) / len(loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43a8f7",
   "metadata": {},
   "source": [
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7fb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(train_loss_epoch, val_loss_epoch, tre_epoch, save_dir='loss_plots'):\n",
    "    \"\"\"\n",
    "    畫出訓練/驗證 loss 曲線與 tre_epoch 變化，並分別儲存圖片。\n",
    "    :param train_loss_epoch: List of training losses per epoch\n",
    "    :param val_loss_epoch: List of validation losses per epoch\n",
    "    :param tre_epoch: List of \"tre\" values per epoch\n",
    "    :param save_dir: Directory to save the plots\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss_epoch, label='Train Loss', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'train_loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(val_loss_epoch, label='Validation Loss', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'val_loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot TRE (or third metric)\n",
    "    plt.figure()\n",
    "    plt.plot(tre_epoch, label='TRE', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('TRE')\n",
    "    plt.title('TRE over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'tre.png'))\n",
    "    plt.close()\n",
    "\n",
    "def  train_main(cfg, save = False):\n",
    "    # config\n",
    "    train_cfg = cfg.train_cfg\n",
    "    dataset_cfg = cfg.dataset_cfg\n",
    "    model_cfg = cfg.model_cfg\n",
    "    device = cfg.device\n",
    "\n",
    "    # dataset\n",
    "    train_dataset = Dataset_TPE(dataset_cfg, mode='train')\n",
    "    val_dataset = Dataset_TPE(dataset_cfg, mode='valid')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_cfg.batch_size, shuffle=True, pin_memory=True,\n",
    "                              drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_cfg.batch_size, shuffle=False, pin_memory=True,\n",
    "                            drop_last=True)\n",
    "\n",
    "    print(train_cfg.optimizer_cfg)\n",
    "    # build model\n",
    "    model = build_model(model_cfg).to(device)\n",
    "\n",
    "    # whether Parallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    if torch.cuda.device_count() == 1:\n",
    "        print(\"there is 1 GPU\")\n",
    "    parameters = model.parameters()\n",
    "\n",
    "    # Parameter information\n",
    "    print('Net\\'s state_dict:')\n",
    "    total_param = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, '\\t', model.state_dict()[param_tensor].size())\n",
    "        total_param += np.prod(model.state_dict()[param_tensor].size())\n",
    "    print('Net\\'s total params:', total_param)\n",
    "\n",
    "    # loss\n",
    "    MSE_criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_cfg = train_cfg.optimizer_cfg\n",
    "    lr_scheduler_cfg = train_cfg.lr_scheduler_cfg\n",
    "    if optimizer_cfg.type == 'adam':\n",
    "        optimizer = optim.Adam(params=parameters,\n",
    "                               lr=optimizer_cfg.lr)\n",
    "    elif optimizer_cfg.type == 'adamw':\n",
    "        optimizer = optim.AdamW(params=parameters,\n",
    "                                lr=optimizer_cfg.lr,\n",
    "                                weight_decay=optimizer_cfg.weight_decay)\n",
    "    elif optimizer_cfg.type == 'sgd':\n",
    "        optimizer = optim.SGD(params=parameters,\n",
    "                              lr=optimizer_cfg.lr,\n",
    "                              momentum=optimizer_cfg.momentum,\n",
    "                              weight_decay=optimizer_cfg.weight_decay)\n",
    "    elif optimizer_cfg.type == 'RMS':\n",
    "        optimizer = optim.RMSprop(params=parameters,\n",
    "                                  lr=optimizer_cfg.lr,\n",
    "                                  momentum=optimizer_cfg.momentum,\n",
    "                                  weight_decay=optimizer_cfg.weight_decay)\n",
    "    else:\n",
    "        raise Exception('No Optimizer！')\n",
    "\n",
    "    # learning schedule\n",
    "    if lr_scheduler_cfg is None:\n",
    "        lr_scheduler = None\n",
    "    elif lr_scheduler_cfg.policy == 'step':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_scheduler_cfg.step_size,\n",
    "                                                       gamma=lr_scheduler_cfg.gamma, last_epoch=-1)\n",
    "    elif lr_scheduler_cfg.policy == 'cos':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, lr_scheduler_cfg.T_0,\n",
    "                                                                            lr_scheduler_cfg.T_mult,\n",
    "                                                                            lr_scheduler_cfg.eta_min)\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    Best_Metric = 0\n",
    "    Min_Metric = 999999999\n",
    "    best_epoch = 0\n",
    "    check_point_dir = '/'.join(train_cfg.check_point_file.split('/')[:-1])\n",
    "\n",
    "    if not os.path.exists(check_point_dir):\n",
    "        os.mkdir(check_point_dir)\n",
    "    train_loss_epoch = []\n",
    "    val_loss_epoch = []\n",
    "    tre_epoch = []\n",
    "    print(\"train_cfg.num_epochs = \", train_cfg.num_epochs)\n",
    "    for epoch in range(1, train_cfg.num_epochs + 1):\n",
    "        print()\n",
    "        print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "        start_time = time.time()\n",
    "        print(f\"It is the {epoch}th epoch...\")\n",
    "\n",
    "        # training\n",
    "        train_loss = train_epoch(model, optimizer, MSE_criterion, train_loader, lr_scheduler, epoch, device)\n",
    "        if epoch % train_cfg.test_interval == 0:\n",
    "            # validate\n",
    "            val_loss = evaluate_model(model, MSE_criterion, val_loader, device, train_cfg, False, epoch)\n",
    "            Best_Metric = val_loss[0]\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        # save model\n",
    "        if Min_Metric > Best_Metric:\n",
    "            Min_Metric = Best_Metric\n",
    "            best_epoch = epoch\n",
    "            torch.save(model, train_cfg.check_point_file)\n",
    "\n",
    "        if epoch == 75 or epoch == 155 or epoch == train_cfg.num_epochs or save == True:\n",
    "            now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_file = train_cfg.check_point_file.split('.')[0] + f'.pth'\n",
    "            torch.save(model, model_file)\n",
    "\n",
    "        # print out\n",
    "        test_end_time = time.time()\n",
    "        run_time = int(test_end_time - start_time)\n",
    "        m, s = divmod(run_time, 60)\n",
    "        time_str = \"{:02d}m{:02d}s\".format(m, s)\n",
    "        out_str = \"The {} epoch is finished, consuming {},\\n\" \\\n",
    "                  \"The loss on training set is {:.6f}；the best epoch is {}, best_metric={:.6f}\" \\\n",
    "            .format(epoch, time_str, sum(train_loss_list) / len(train_loss_list), best_epoch, Min_Metric)\n",
    "        print(out_str)\n",
    "        train_loss_epoch.append(sum(train_loss_list) / len(train_loss_list))\n",
    "        val_loss_epoch.append(Best_Metric)\n",
    "        tre_epoch.append(val_loss[3])\n",
    "    # draw train_loss_epoch & val_loss_epoch\n",
    "    plot_loss_curves(train_loss_epoch, val_loss_epoch, tre_epoch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb2252",
   "metadata": {},
   "source": [
    "# Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03d9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "\n",
    "def test_main(cfg):\n",
    "    # config\n",
    "    test_cfg = cfg.test_cfg\n",
    "    dataset_cfg = cfg.dataset_cfg\n",
    "    device = cfg.device\n",
    "    model_cfg = cfg.model_cfg\n",
    "\n",
    "    # dataset\n",
    "    test_dataset = Dataset_TPE(dataset_cfg, mode='test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    \n",
    "    # loss\n",
    "    model = build_model(model_cfg).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    MSE_criterion = nn.MSELoss().to(device)\n",
    "    model_file = test_cfg.check_point_file\n",
    "    checkpoint = torch.load(model_file, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint.state_dict())\n",
    "    print(\"test_loader.dataset\",test_loader.dataset)\n",
    "    test_loss = evaluate_model(model, MSE_criterion, test_loader, device, test_cfg, False)\n",
    "    print('The loss on test set is {:.6f}'.format(test_loss[0]))\n",
    "\n",
    "    now_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"{now_str} test...\\n\")\n",
    "    \n",
    "    # ablation\n",
    "    \n",
    "    M = 15\n",
    "    importances = []\n",
    "    for i in range(7, M):\n",
    "        loss_i = evaluate_model_with_custom_YD(model, test_loader, MSE_criterion, device, i, test_cfg)\n",
    "        \n",
    "        importance = loss_i[0] - test_loss[0]\n",
    "        importances.append(importance)\n",
    "    print(importances)\n",
    "    # 畫出 bar chart\n",
    "    import matplotlib.pyplot as plt\n",
    "    feature_names = [\"workday\", \"rush hour\" ,\"weekend\" ,\"holiday\" ,\"make-up workday\" ,\"day of week\" ,\"weather\"]\n",
    "    plt.bar(range(7), importances)\n",
    "    plt.xticks(ticks=range(7), labels=feature_names, rotation=45, ha='right')\n",
    "    plt.xlabel(\"External Feature Index\")\n",
    "    plt.ylabel(\"Importance (Loss Increase)\")\n",
    "    plt.title(\"Feature Importance by Ablation\")\n",
    "    plt.savefig(\"feature_importance.png\", dpi=300, bbox_inches='tight')  # 存成 PNG\n",
    "    plt.close()\n",
    "    # # 假設 model 已經 load 完畢\n",
    "    # model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "\n",
    "    # # 儲存資料夾\n",
    "    # save_dir = \"weights_txt\"\n",
    "    # os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # for name, param in model_state.items():\n",
    "    #     if 'weight' in name:\n",
    "    #         weight_np = param.detach().cpu().numpy()\n",
    "    #         save_path = os.path.join(save_dir, f\"{name.replace('.', '_')}.txt\")\n",
    "    #         np.savetxt(save_path, weight_np.flatten(), fmt=\"%.6f\")\n",
    "    #         print(f\"Saved {name} to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f7625",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac4fc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from importlib import import_module\n",
    "\n",
    "from addict import Dict\n",
    "\n",
    "# from .misc import collections_abc\n",
    "\n",
    "\n",
    "class ConfigDict(Dict):\n",
    "    def __missing__(self, name):\n",
    "        raise KeyError(name)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            value = super(ConfigDict, self).__getattr__(name)\n",
    "        except KeyError:\n",
    "            ex = AttributeError(\n",
    "                \"'{}' object has no attribute '{}'\".format(\n",
    "                    self.__class__.__name__, name\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            ex = e\n",
    "        else:\n",
    "            return value\n",
    "        raise ex\n",
    "\n",
    "\n",
    "def add_args(parser, cfg, prefix=\"\"):\n",
    "    for k, v in cfg.items():\n",
    "        if isinstance(v, str):\n",
    "            parser.add_argument(\"--\" + prefix + k)\n",
    "        elif isinstance(v, int):\n",
    "            parser.add_argument(\"--\" + prefix + k, type=int)\n",
    "        elif isinstance(v, float):\n",
    "            parser.add_argument(\"--\" + prefix + k, type=float)\n",
    "        elif isinstance(v, bool):\n",
    "            parser.add_argument(\"--\" + prefix + k, action=\"store_true\")\n",
    "        elif isinstance(v, dict):\n",
    "            add_args(parser, v, k + \".\")\n",
    "        # elif isinstance(v, collections_abc.Iterable):\n",
    "        #     parser.add_argument(\"--\" + prefix + k, type=type(v[0]), nargs=\"+\")\n",
    "        else:\n",
    "            print(\"connot parse key {} of type {}\".format(prefix + k, type(v)))\n",
    "    return parser\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"A facility for config and config files.\n",
    "    It supports common file formats as configs: python/json/yaml. The interface\n",
    "    is the same as a dict object and also allows access config values as\n",
    "    attributes.\n",
    "    Example:\n",
    "        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n",
    "        >>> cfg.a\n",
    "        1\n",
    "        >>> cfg.b\n",
    "        {'b1': [0, 1]}\n",
    "        >>> cfg.b.b1\n",
    "        [0, 1]\n",
    "        >>> cfg = Config.fromfile('tests/data/config/a.py')\n",
    "        >>> cfg.filename\n",
    "        \"/home/kchen/projects/torchie/tests/data/config/a.py\"\n",
    "        >>> cfg.item4\n",
    "        'test'\n",
    "        >>> cfg\n",
    "        \"Config [path: /home/kchen/projects/torchie/tests/data/config/a.py]: \"\n",
    "        \"{'item1': [1, 2], 'item2': {'a': 0}, 'item3': True, 'item4': 'test'}\"\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fromfile(filename):\n",
    "        filename = osp.abspath(osp.expanduser(filename))\n",
    "        if not osp.isfile(filename):\n",
    "            raise FileNotFoundError('file \"{}\" does not exist'.format(filename))\n",
    "\n",
    "        if filename.endswith(\".py\"):\n",
    "            module_name = osp.basename(filename)[:-3]\n",
    "            if \".\" in module_name:\n",
    "                raise ValueError(\"Dots are not allowed in config file path.\")\n",
    "            config_dir = osp.dirname(filename)\n",
    "            sys.path.insert(0, config_dir)\n",
    "            mod = import_module(module_name)\n",
    "            sys.path.pop(0)\n",
    "            cfg_dict = {\n",
    "                name: value\n",
    "                for name, value in mod.__dict__.items()\n",
    "                if not name.startswith(\"__\")\n",
    "            }\n",
    "        elif filename.endswith((\".yml\", \".yaml\", \".json\")):\n",
    "            import torchie\n",
    "\n",
    "            cfg_dict = torchie.load(filename)\n",
    "        else:\n",
    "            raise IOError(\"Only py/yml/yaml/json type are supported now!\")\n",
    "        return Config(cfg_dict, filename=filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def auto_argparser(description=None):\n",
    "        \"\"\"Generate argparser from config file automatically (experimental)\n",
    "        \"\"\"\n",
    "        partial_parser = ArgumentParser(description=description)\n",
    "        partial_parser.add_argument(\"config\", help=\"config file path\")\n",
    "        cfg_file = partial_parser.parse_known_args()[0].config\n",
    "        cfg = Config.fromfile(cfg_file)\n",
    "        parser = ArgumentParser(description=description)\n",
    "        parser.add_argument(\"config\", help=\"config file path\")\n",
    "        add_args(parser, cfg)\n",
    "        return parser, cfg\n",
    "\n",
    "    def __init__(self, cfg_dict=None, filename=None):\n",
    "        if cfg_dict is None:\n",
    "            cfg_dict = dict()\n",
    "        elif not isinstance(cfg_dict, dict):\n",
    "            raise TypeError(\n",
    "                \"cfg_dict must be a dict, but got {}\".format(type(cfg_dict))\n",
    "            )\n",
    "\n",
    "        super(Config, self).__setattr__(\"_cfg_dict\", ConfigDict(cfg_dict))\n",
    "        super(Config, self).__setattr__(\"_filename\", filename)\n",
    "        if filename:\n",
    "            with open(filename, \"r\") as f:\n",
    "                super(Config, self).__setattr__(\"_text\", f.read())\n",
    "        else:\n",
    "            super(Config, self).__setattr__(\"_text\", \"\")\n",
    "\n",
    "    @property\n",
    "    def filename(self):\n",
    "        return self._filename\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        return self._text\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Config (path: {}): {}\".format(self.filename, self._cfg_dict.__repr__())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._cfg_dict)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._cfg_dict, name)\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self._cfg_dict.__getitem__(name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, dict):\n",
    "            value = ConfigDict(value)\n",
    "        self._cfg_dict.__setattr__(name, value)\n",
    "\n",
    "    def __setitem__(self, name, value):\n",
    "        if isinstance(value, dict):\n",
    "            value = ConfigDict(value)\n",
    "        self._cfg_dict.__setitem__(name, value)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._cfg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee12c91",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "477bfd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config filename: DeepMeshCity_TPE_flow_config.py\n",
      "random seed is 6666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "cfg_filename = \"DeepMeshCity_TPE_flow_config.py\"  # config filename\n",
    "data = \"TPE\"\n",
    "task = \"flow\"  # flow,density\n",
    "\n",
    "# read config\n",
    "cfg = Config.fromfile('./Config/' + data + '/' + task + '/' + cfg_filename)\n",
    "print(\"config filename: \" + str(cfg_filename))\n",
    "\n",
    "set_seed(cfg.random_seed)\n",
    "print(\"random seed is {}\".format(cfg.random_seed))\n",
    "\n",
    "split_save_dir = cfg.train_cfg.check_point_file.split('/')\n",
    "save_dir = os.path.join(split_save_dir[0], os.path.join(split_save_dir[1], split_save_dir[2]))\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "if not os.path.exists(cfg.train_cfg.gen_frm_dir):\n",
    "    os.makedirs(cfg.train_cfg.gen_frm_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce174542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'input_length': 6,\n",
       " 'num_epochs': 5,\n",
       " 'max_value': 32,\n",
       " 'test_interval': 1,\n",
       " 'num_save_samples': 3,\n",
       " 'optimizer_cfg': {'type': 'adamw', 'lr': 0.001, 'weight_decay': 0.0005},\n",
       " 'lr_scheduler_cfg': {'policy': 'cos',\n",
       "  'T_0': 5,\n",
       "  'T_mult': 2,\n",
       "  'eta_min': 1e-05},\n",
       " 'check_point_file': 'checkpoint/TPE/flow/DeepMeshCity.pth',\n",
       " 'gen_frm_dir': 'Results/TPE/flow/DeepMeshCity'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.train_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d00cbcc",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "565b9f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7377, 6, 1, 28, 28) (7377, 1, 1, 28, 28) (7377, 1, 1, 28, 28) (7377, 1, 28, 28) (7377, 784, 15) 32\n",
      "(1053, 6, 1, 28, 28) (1053, 1, 1, 28, 28) (1053, 1, 1, 28, 28) (1053, 1, 28, 28) (1053, 784, 15) 32\n",
      "{'type': 'adamw', 'lr': 0.001, 'weight_decay': 0.0005}\n",
      "Net's state_dict:\n",
      "meta_learn.0.weight \t torch.Size([10, 11760])\n",
      "meta_learn.0.bias \t torch.Size([10])\n",
      "meta_learn.2.weight \t torch.Size([784, 10])\n",
      "meta_learn.2.bias \t torch.Size([784])\n",
      "SACGL_blocks.0.0.layer_q.weight \t torch.Size([64, 32, 1, 1])\n",
      "SACGL_blocks.0.0.layer_q.bias \t torch.Size([64])\n",
      "SACGL_blocks.0.0.layer_k.weight \t torch.Size([64, 32, 1, 1])\n",
      "SACGL_blocks.0.0.layer_k.bias \t torch.Size([64])\n",
      "SACGL_blocks.0.0.layer_v.weight \t torch.Size([64, 32, 1, 1])\n",
      "SACGL_blocks.0.0.layer_v.bias \t torch.Size([64])\n",
      "SACGL_blocks.0.0.layer_f.weight \t torch.Size([32, 64, 1, 1])\n",
      "SACGL_blocks.0.0.layer_f.bias \t torch.Size([32])\n",
      "SACGL_blocks.0.1.conv_x.0.weight \t torch.Size([256, 2, 3, 3])\n",
      "SACGL_blocks.0.1.conv_x.0.bias \t torch.Size([256])\n",
      "SACGL_blocks.0.1.conv_x.1.weight \t torch.Size([256])\n",
      "SACGL_blocks.0.1.conv_x.1.bias \t torch.Size([256])\n",
      "SACGL_blocks.0.1.conv_c.0.weight \t torch.Size([128, 64, 3, 3])\n",
      "SACGL_blocks.0.1.conv_c.0.bias \t torch.Size([128])\n",
      "SACGL_blocks.0.1.conv_c.1.weight \t torch.Size([128])\n",
      "SACGL_blocks.0.1.conv_c.1.bias \t torch.Size([128])\n",
      "SACGL_blocks.0.1.conv_m.0.weight \t torch.Size([128, 64, 3, 3])\n",
      "SACGL_blocks.0.1.conv_m.0.bias \t torch.Size([128])\n",
      "SACGL_blocks.0.1.conv_m.1.weight \t torch.Size([128])\n",
      "SACGL_blocks.0.1.conv_m.1.bias \t torch.Size([128])\n",
      "SACGL_blocks.0.1.conv_o_c.0.weight \t torch.Size([64, 128, 1, 1])\n",
      "SACGL_blocks.0.1.conv_o_c.0.bias \t torch.Size([64])\n",
      "SACGL_blocks.0.1.conv_o_c.1.weight \t torch.Size([64])\n",
      "SACGL_blocks.0.1.conv_o_c.1.bias \t torch.Size([64])\n",
      "SACGL_blocks.0.1.conv_last.weight \t torch.Size([64, 128, 1, 1])\n",
      "SACGL_blocks.0.1.conv_last.bias \t torch.Size([64])\n",
      "SACGL_blocks.1.0.layer_q.weight \t torch.Size([64, 1024, 1, 1])\n",
      "SACGL_blocks.1.0.layer_q.bias \t torch.Size([64])\n",
      "SACGL_blocks.1.0.layer_k.weight \t torch.Size([64, 1024, 1, 1])\n",
      "SACGL_blocks.1.0.layer_k.bias \t torch.Size([64])\n",
      "SACGL_blocks.1.0.layer_v.weight \t torch.Size([64, 1024, 1, 1])\n",
      "SACGL_blocks.1.0.layer_v.bias \t torch.Size([64])\n",
      "SACGL_blocks.1.0.layer_f.weight \t torch.Size([1024, 64, 1, 1])\n",
      "SACGL_blocks.1.0.layer_f.bias \t torch.Size([1024])\n",
      "SACGL_blocks.1.1.conv_x.0.weight \t torch.Size([256, 64, 3, 3])\n",
      "SACGL_blocks.1.1.conv_x.0.bias \t torch.Size([256])\n",
      "SACGL_blocks.1.1.conv_x.1.weight \t torch.Size([256])\n",
      "SACGL_blocks.1.1.conv_x.1.bias \t torch.Size([256])\n",
      "SACGL_blocks.1.1.conv_c.0.weight \t torch.Size([128, 64, 3, 3])\n",
      "SACGL_blocks.1.1.conv_c.0.bias \t torch.Size([128])\n",
      "SACGL_blocks.1.1.conv_c.1.weight \t torch.Size([128])\n",
      "SACGL_blocks.1.1.conv_c.1.bias \t torch.Size([128])\n",
      "SACGL_blocks.1.1.conv_m.0.weight \t torch.Size([128, 64, 3, 3])\n",
      "SACGL_blocks.1.1.conv_m.0.bias \t torch.Size([128])\n",
      "SACGL_blocks.1.1.conv_m.1.weight \t torch.Size([128])\n",
      "SACGL_blocks.1.1.conv_m.1.bias \t torch.Size([128])\n",
      "SACGL_blocks.1.1.conv_o_c.0.weight \t torch.Size([64, 128, 1, 1])\n",
      "SACGL_blocks.1.1.conv_o_c.0.bias \t torch.Size([64])\n",
      "SACGL_blocks.1.1.conv_o_c.1.weight \t torch.Size([64])\n",
      "SACGL_blocks.1.1.conv_o_c.1.bias \t torch.Size([64])\n",
      "SACGL_blocks.1.1.conv_last.weight \t torch.Size([64, 128, 1, 1])\n",
      "SACGL_blocks.1.1.conv_last.bias \t torch.Size([64])\n",
      "Output_module.0.weight \t torch.Size([64, 64, 1, 1])\n",
      "Output_module.2.weight \t torch.Size([1, 64, 1, 1])\n",
      "Net's total params: 885498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbb921921/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_cfg.num_epochs =  5\n",
      "\n",
      "2025-07-11 00:49:11\n",
      "It is the 1th epoch...\n",
      "The learning rate of the 1th epoch：0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [04:23,  2.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# train, test, ablation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m TRE_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 148\u001b[0m, in \u001b[0;36mtrain_main\u001b[0;34m(cfg, save)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth epoch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMSE_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m train_cfg\u001b[38;5;241m.\u001b[39mtest_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# validate\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate_model(model, MSE_criterion, val_loader, device, train_cfg, \u001b[38;5;28;01mFalse\u001b[39;00m, epoch)\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, loss_func, dataloader, lr_scheduler, epoch, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m next_frame \u001b[38;5;241m=\u001b[39m model(xc, xp, xt, yd)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(next_frame, ys)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m loss_item \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/work/DeepMeshCity-1/deepmeshcity/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mode = \"train\"  # train, test, ablation\n",
    "TRE_draw = False\n",
    "train_main(cfg, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb42b63",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b340d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1719, 6, 1, 28, 28) (1719, 1, 1, 28, 28) (1719, 1, 1, 28, 28) (1719, 1, 28, 28) (1719, 13) 1292.0\n",
      "checkpoint/TPE/flow/DeepMeshCity.pth\n",
      "2025-05-16 01:27:03 test...\n",
      "mse per frame: 144.23587036132812, rmse per frame: 12.0098237991333, mae per frame: 3.602595090866089, tre : 27.40910485656527 %\n",
      "The loss on test set is 144.235870\n",
      "2025-05-16 01:27:32 test...\n",
      "Saved meta_learn.0.weight to weights_txt\\meta_learn_0_weight.txt\n",
      "Saved meta_learn.2.weight to weights_txt\\meta_learn_2_weight.txt\n",
      "Saved SACGL_blocks.0.0.layer_q.weight to weights_txt\\SACGL_blocks_0_0_layer_q_weight.txt\n",
      "Saved SACGL_blocks.0.0.layer_k.weight to weights_txt\\SACGL_blocks_0_0_layer_k_weight.txt\n",
      "Saved SACGL_blocks.0.0.layer_v.weight to weights_txt\\SACGL_blocks_0_0_layer_v_weight.txt\n",
      "Saved SACGL_blocks.0.0.layer_f.weight to weights_txt\\SACGL_blocks_0_0_layer_f_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_x.0.weight to weights_txt\\SACGL_blocks_0_1_conv_x_0_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_x.1.weight to weights_txt\\SACGL_blocks_0_1_conv_x_1_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_c.0.weight to weights_txt\\SACGL_blocks_0_1_conv_c_0_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_c.1.weight to weights_txt\\SACGL_blocks_0_1_conv_c_1_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_m.0.weight to weights_txt\\SACGL_blocks_0_1_conv_m_0_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_m.1.weight to weights_txt\\SACGL_blocks_0_1_conv_m_1_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_o_c.0.weight to weights_txt\\SACGL_blocks_0_1_conv_o_c_0_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_o_c.1.weight to weights_txt\\SACGL_blocks_0_1_conv_o_c_1_weight.txt\n",
      "Saved SACGL_blocks.0.1.conv_last.weight to weights_txt\\SACGL_blocks_0_1_conv_last_weight.txt\n",
      "Saved SACGL_blocks.1.0.layer_q.weight to weights_txt\\SACGL_blocks_1_0_layer_q_weight.txt\n",
      "Saved SACGL_blocks.1.0.layer_k.weight to weights_txt\\SACGL_blocks_1_0_layer_k_weight.txt\n",
      "Saved SACGL_blocks.1.0.layer_v.weight to weights_txt\\SACGL_blocks_1_0_layer_v_weight.txt\n",
      "Saved SACGL_blocks.1.0.layer_f.weight to weights_txt\\SACGL_blocks_1_0_layer_f_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_x.0.weight to weights_txt\\SACGL_blocks_1_1_conv_x_0_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_x.1.weight to weights_txt\\SACGL_blocks_1_1_conv_x_1_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_c.0.weight to weights_txt\\SACGL_blocks_1_1_conv_c_0_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_c.1.weight to weights_txt\\SACGL_blocks_1_1_conv_c_1_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_m.0.weight to weights_txt\\SACGL_blocks_1_1_conv_m_0_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_m.1.weight to weights_txt\\SACGL_blocks_1_1_conv_m_1_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_o_c.0.weight to weights_txt\\SACGL_blocks_1_1_conv_o_c_0_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_o_c.1.weight to weights_txt\\SACGL_blocks_1_1_conv_o_c_1_weight.txt\n",
      "Saved SACGL_blocks.1.1.conv_last.weight to weights_txt\\SACGL_blocks_1_1_conv_last_weight.txt\n",
      "Saved Output_module.0.weight to weights_txt\\Output_module_0_weight.txt\n",
      "Saved Output_module.2.weight to weights_txt\\Output_module_2_weight.txt\n"
     ]
    }
   ],
   "source": [
    "test_main(cfg=cfg) # no external 75 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d657da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 6, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 28, 28) (168, 14) 1292.0\n",
      "2025-06-06 01:41:31 test...\n",
      "The loss on test set is 147.436371\n",
      "2025-06-06 01:41:46 test...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_main(cfg=cfg) # with external 10 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 6, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 28, 28) (168, 14) 1292.0\n",
      "2025-06-06 09:38:21 test...\n",
      "The loss on test set is 229.011154\n",
      "2025-06-06 09:38:34 test...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_main(cfg=cfg) # with external 150 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437afe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 6, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 28, 28) (168, 14) 1292.0\n",
      "2025-06-06 10:09:53 test...\n",
      "The loss on test set is 229.011154\n",
      "2025-06-06 10:10:07 test...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_main(cfg=cfg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00737f",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f985d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 6, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 1, 28, 28) (168, 1, 28, 28) (168, 784, 32) 32\n",
      "test_loader.dataset <__main__.Dataset_TPE object at 0x45c5e87f0>\n",
      "2025-07-07 15:47:17 test...\n",
      "batch 0\n",
      "valid_mask shape: torch.Size([32, 1, 28, 28])\n",
      "pred shape: torch.Size([32, 1, 28, 28]) ys shape: torch.Size([32, 1, 28, 28])\n",
      "batch 1\n",
      "valid_mask shape: torch.Size([32, 1, 28, 28])\n",
      "pred shape: torch.Size([32, 1, 28, 28]) ys shape: torch.Size([32, 1, 28, 28])\n",
      "batch 2\n",
      "valid_mask shape: torch.Size([32, 1, 28, 28])\n",
      "pred shape: torch.Size([32, 1, 28, 28]) ys shape: torch.Size([32, 1, 28, 28])\n",
      "batch 3\n",
      "valid_mask shape: torch.Size([32, 1, 28, 28])\n",
      "pred shape: torch.Size([32, 1, 28, 28]) ys shape: torch.Size([32, 1, 28, 28])\n",
      "batch 4\n",
      "valid_mask shape: torch.Size([32, 1, 28, 28])\n",
      "pred shape: torch.Size([32, 1, 28, 28]) ys shape: torch.Size([32, 1, 28, 28])\n",
      "sum_tre: 611.37726\n",
      "count: 334\n",
      "mse per frame: 0.0895157903432846, rmse per frame: 0.2991918921470642, mae per frame: 0.08874036371707916, avg_tre : 1.8304708003997803 %\n",
      "The loss on test set is 0.089516\n",
      "2025-07-07 15:53:10 test...\n",
      "\n",
      "evaluate_model_with_custom_YD: 7\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "evaluate_model_with_custom_YD: 8\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "evaluate_model_with_custom_YD: 9\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "evaluate_model_with_custom_YD: 10\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "evaluate_model_with_custom_YD: 11\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "evaluate_model_with_custom_YD: 12\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "evaluate_model_with_custom_YD: 13\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "YD shape: torch.Size([32, 784, 32])\n",
      "[np.float32(6.2048435e-05), np.float32(5.6102872e-05), np.float32(5.300343e-05), np.float32(6.148219e-05), np.float32(5.2034855e-05), np.float32(5.5007637e-05), np.float32(6.3978136e-05)]\n"
     ]
    }
   ],
   "source": [
    "TRE_draw = True\n",
    "test_main(cfg=cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d5d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepmeshcity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
